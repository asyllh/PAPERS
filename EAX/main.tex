\documentclass[authoryear]{elsarticle}
\input{includes/preamble.tex}
\begin{document}
	
\begin{frontmatter}
\title{Evolutionary Methods for the Score-Constrained Packing Problem}
\author{Asyl L. Hawa}
\author{Rhyd Lewis}
\author{Jonathan M. Thompson}
\address{School of Mathematics, Cardiff University, Senghennydd Road, Cardiff, UK}
\begin{abstract}
\note{Type of packing problem in which order and orientation of items is important. Paper investigates heuristics, EA, postoptimisation.} 
\end{abstract}	
\end{frontmatter}

%--------------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
\noindent Many problems in operational research and discrete mathematics involve the grouping of elements into subsets. These types of problems can be seen in areas such as scheduling \citep{thompson1998, carter1996}, frequency assignment \citep{aardal2007}, graph colouring \citep{lewis2015, malaguti2008}, and load balancing \citep{rekiek1999}, as well as in practical problems in computer science such as table formatting, prepaging, and file allocation \citep{garey1972}. Formally, given a set $\mathcal{I}$ of $n$ elements, the aim in such problems is to produce a partition of subsets $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{j=1}^{k} S_j &= \mathcal{I}, & \label{eqn:allpacked}\\[3pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1, 2,\dotsc,k\}, \hspace{1mm} i \neq j, \text{ and}\label{eqn:nointersect}\\[3pt]
	S_j &\in \mathcal{F} & &\forall \hspace{1mm} j \in \{1,2,\dotsc,k\}.\label{eqn:feasible}
	\end{alignat}
\end{subequations}

\noindent Here, \eqref{eqn:allpacked} and \eqref{eqn:nointersect} state the requirement that every element must be in exactly one of the $k$ subsets and \eqref{eqn:feasible} specifies that each subset $S_i \in \mathcal{S}$ must be feasible, where $\mathcal{F}$ is used to denote the set of all feasible subsets of elements in $\mathcal{I}$. The notion of feasibility is dependent on the particular constraints of the given problem. For example, in the graph colouring problem where vertices on a graph must be assigned colours such that no two adjacent vertices are in the same colour class, $\mathcal{F}$ contains all possible independent sets of vertices, whilst for the classical one-dimensional bin-packing problem (BPP) which requires a set of items of varying sizes to be packed into the fewest number of bins of fixed capacity, a bin $S_i$ is feasible only if the sum of its item sizes is less than or equal to the bin's capacity.

The focus of this paper is on a special type of packing problem that occurs in the packaging industry, where flat rectangular items of varying widths are to be cut and scored from fixed-length strips of cardboard which are then folded into boxes. This problem was originally introduced as an open-combinatorial problem by \citeauthor{goulimis2004} in 2004, and subsequently studied by \citet{lewis2011}, \citet{becker2015}, and \citet{hawa2018}.

Consider a set $\mathcal{I}$ of $n$ rectangular items of fixed height $H$. Each item $i \in \mathcal{I}$ has width $w_i \in \mathbb{Z}^+$, and is marked with two vertical score lines in predetermined places. The distance between each score line and the nearest edge of the item are the score widths, $a_i, b_i \in \mathbb{Z}^+$ (where w.l.o.g. $a_i \leq b_i$). An example of an item $i$ with these dimensions is provided in Fig.~\ref{fig:itemsdimknives}. In the industrial process described by Goulimis, pairs of knives mounted on a bar simultaneously cut along the score lines of two adjacent items, making it easier to fold the cardboard at a later stage; however due to the manner in which the machine is designed, the knives in each pair must maintain a set distance from one another, a so-called ``minimum scoring distance'' $\tau \in \mathbb{Z}^+$ (approximately 70mm in practice). For the knives to score all of the items in the correct locations, the distance between two score lines of adjacent items must therefore equal or exceed the minimum scoring distance. Hence, the following \emph{vicinal sum constraint} must be fulfilled:
\begin{equation}
	\textbf{r}(i) + \textbf{l}(i+1) \geq \tau \quad \forall \hspace{1mm} i \in \{1,2,\dotsc,|S|- 1\},
	\label{eqn:vsc}
\end{equation}

\noindent where \textbf{l}($i$) and \textbf{r}($i$) denote the left- and right-hand score widths of the $i$th item in bin $S$. Clearly if this constraint is satisfied the distance between the score lines will be sufficient for the knives to be able to cut appropriately. Figure~\ref{fig:itemsdimknives} shows how adjacent items are scored simultaneously by pairs of knives. Although the vicinal sum constraint is met between items A and B, the full alignment of all three items is infeasible as the sum of the adjacent score widths of items B and C is less than the minimum scoring distance $\tau$, and so the knives are unable to move close enough together to score the lines in the required locations.
\begin{figure}[H]	
	\centering
	\includestandalone[width=0.7\textwidth]{figures/itemsdimknives}
	\caption{Dimensions of an item $i$ marked with dashed score lines, and an example packing showing both feasible and infeasible alignments of three items to be scored by pairs of knives. Here, the minimum scoring distance $\tau = 70$.}	
	\label{fig:itemsdimknives}
\end{figure}

\noindent The remainder of this section will formally define the single bin problem and the corresponding multi-bin version, the Score-Constrained Packing Problem. In the next section, we will provide a brief overview of a polynomial-time algorithm used to solve the single bin problem. Section~\ref{sec:heur} will explain the difficulties associated with the SCPP and analyse heuristics currently in literature. An evolutionary algorithm for the SCPP is presented in Section~\ref{sec:ea}, along with results from rigorous experiments. Section~\ref{sec:cmsa} details a hybrid metaheuritic method combined with an exact procedure involving a recursive backtracking algorithm to improve upon results obtained from the EA, and finally Section~\ref{sec:conclusion} concludes the paper and discusses outcome and possible directions for further work.

\subsection{Problem Definitions}
\label{sub:intro}

\noindent Let us now formally define the main problem to be investigated in this paper:

\begin{definition}
	Let $\mathcal{I}$ be a set of $n$ rectangular items of height $H$ with varying widths $w_i$ and score widths $a_i, b_i$ $\forall$ $i \in \mathcal{I}$. Given a minimum scoring distance $\tau$, the \emph{Score-Constrained Packing Problem (SCPP)} involves packing the items from left to right into the fewest number of $H \times W$ bins such that (a) the vicinal sum constraint is satisifed in each bin and (b) no bin is overfilled.
	\label{defn:scpp}
\end{definition}	

\noindent Each item $i \in \mathcal{I}$ can be packed into a bin in either a regular orientation, denoted $(a_i, b_i)$, where the smaller score width $a_i$ is on the left-hand side of item $i$, or a rotated orientation $(b_i, a_i)$, where the larger score width $b_i$ is on the left-hand side. Thus, there is the additional packing problem within each individual bin, defined as follows:

\begin{definition}
	Let $\mathcal{I}' \subseteq \mathcal{I}$ be a set of rectangular items whose total width $A(\mathcal{I}') = \sum_{i \in \mathcal{I}'} w_i$ is less than or equal to the bin width $W$. Then, given a minimum scoring distance $\tau$, the \emph{Score-Constrained Packing Sub-Problem (SubSCP)} consists of finding an ordering and orientation of the items in $\mathcal{I}'$ in the bin such that the vicinal sum constraint is satisfied.
	\label{defn:subscp}
\end{definition}


\noindent For example, in Fig.~\ref{fig:itemsdimknives}, observe that a feasible alignment of the three items in a single bin can be obtained by rotating item C.\footnote{Note that the outermost score widths in each bin are disregarded as they are not adjacent to any other items.} As there are $2^{n-1} n!$ distinct orderings of $n$ items, it is clear that enumerative methods are not suitable. Figure~\ref{fig:bppvscpp} shows feasible solutions for a set of items $\mathcal{I}$ as an instance of the BPP and the SCPP. For the SCPP, an extra bin is required to accommodate all items whilst fulfilling the vicinal sum constraint. Note that the solution produced for the BPP is not feasible for the SCPP in this case as the constraint is violated at least once in every bin. Thus the BPP can be seen as a special case of the SCPP when $\tau=0$, as the vicinal sum constraint will always be satisfied.

\begin{figure}[H]
	\centering	
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bpp}
		\caption{BPP}
		\label{fig:bpp}
	\end{subfigure} \hspace{15mm}
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/scpp}
		\caption{SCPP}
		\label{fig:scpp}
	\end{subfigure}
	\caption{Solutions for the BPP and SCPP using the same set $\mathcal{I}$ of 10 items and $W = 10$. For the SCPP, $\tau = 70$. The red score lines on the solution for the BPP show the vicinal sum constraint violations if it were to be used as a solution for the SCPP.}	
	\label{fig:bppvscpp}
\end{figure}

\noindent One interesting problem related to the BPP is the Trapezoid Packing Problem (TPP), initially investigated by \citet{lewis2011}, where trapezoids are to be packed into bins so as to minimise the number of bins required, whilst also attempting to reduce the amount of triangular waste between adjacent trapezoids. Another problem similar to the BPP is the cutting stock problem (CSP), which involves cutting large pieces of material into smaller piece whilst minimising material wasted. One particular case, described by \citet{garraffa2016}, considers sequence-dependent cut-losses (SDCL). Here, rectangular items of varying lengths are to be cut from strips of material of fixed lengths; however the type of cutting machine used results in material loss between items during the cutting process. The amount of loss can vary between different items, and is also dependent on the order of the items, i.e. a cut loss between two adjacent items A and B, with A packed first, may not necessarily be equal to the cut loss that arises when B is packed first. Hence, the CSP-SDCL involves packing the items into the fewest number of bins such that the sum of item lengths \emph{and} the sum of cut losses between all adjacent items in each bin does not exceed the bin capacity.

As with the TPP and CSP-SDCL, the SCPP not only involves deciding which bin each item should be packed into, but also, unlike the BPP, \emph{how} the items should be packed -- that is, determining the order and orientation of items within each bin. One specific difference, however, concerns the feasibility of individual bins. In the TPP, although clearly not optimal, it is still legal to place trapezoids with opposite angles, i.e. `$\backslash$' and `/', alongside one another. Likewise in the CSP-SDCL, two items with a large cut loss between them can still be packed alongside one another if necessary. Both of these problems allow items to be packed in \emph{any} order and orientation as long as the bins are not overfilled. In contrast, the SCPP possesses the strong vicinal sum constraint which if violated immediately causes an alignment of items in a bin to be invalid, thus rendering the entire solution infeasible.

%--------------------------------------------------------------------------------------
\section{Solving the SubSCP}
\label{sec:ahc}
\noindent Firstly, consider the following sequencing problem originally defined by \citet{hawa2018}:

\begin{definition} % COP
	\label{defn:cop}
	Let $\mathcal{M}$ be a multiset of unordered pairs of integers $\mathcal{M} = \{\{a_1, b_1\}, \{a_2, b_2\},\dotsc,\{a_n, b_n\}\}$, and let $\mathcal{T}$ be a sequence of the elements of $\mathcal{M}$ in which each pair is a tuple. Given a fixed value $\tau \in \mathbb{Z}^+$, the Constrained Ordering Problem (COP) consists of finding a solution $\mathcal{T}$ such that the sum of adjacent values from different tuples is greater than or equal to $\tau$.
\end{definition}

\noindent For example, given the instance $\mathcal{M} = \{\{4,21\}, \{9,53\}, \{13,26\}, \{17,29\}, \{32,39\}, \{35,41\}, \{44,57\}, \{48,61\} \}$ and $\tau = 70$, one possible solution is $\mathcal{T} = \langle(4,21), (53,9), (61,48), (26,13), (57,44), (32,39), (35,41), (29,17)\rangle$. It is evident that the COP is in fact equivalent to the SubSCP, whereby each pair in $\mathcal{M}$ can be seen as an item $i$ represented by its score widths $a_i, b_i$, and the constraint value $\tau$ is the minimum scoring distance. It follows that the requirement for the sum of adjacent values to exceed $\tau$ corresponds to the vicinal sum constraint \eqref{eqn:vsc}.

In this section we present the Alternating Hamiltonian Construction (AHC) algorithm, a polynomial-time algorithm for solving the COP, and hence the SubSCP. The underlying algorithm was originally proposed by \citet{becker2010} and determines whether a feasible solution exists for a given instance. This was then extended by \citet{hawa2018} so that, if a solution does indeed exist AHC, is able to construct the final solution. Here, we further simplify and increase the efficiency of AHC.


We begin by modelling an instance $\mathcal{M}$ of the COP graphically. For each pair $\{a_i, b_i\} \in \mathcal{M}$, two vertices $u, v$ with weights $w(u) = a_i$, $w(v) = b_i$ are created, together with a ``blue'' edge $\{u, v\}$. Such vertices formed from a pair in $\mathcal{M}$ are referred to as \textit{partners}. This gives a vertex-weighted graph $G$ comprising $n$ components. Without loss of generality, we also assume that the vertices $\{v_1,\dotsc,v_{2n}\}$ are labelled in weight order such that $w(v_i) \leq w(v_{i+1})$.

To prevent executing the algorithm unnecessarily, a basic preliminary test is first performed. Of the $2n$ vertices, suppose vertices $v_1$ and $v_2$ are placed on the ends of the sequence. Clearly, if vertices $v_3$ and $v_{2n}$ do not meet the vicinal sum constraint, i.e. $w(v_3) + w(v_{2n}) < \tau$, then there cannot exist a feasible ordering of all elements in $\mathcal{M}$. Note that a positive outcome from this test does not necessarily imply that a feasible solution exists for the instance, however a negative outcome confirms the non-existence of a solution.

If $\mathcal{M}$ has not yet been deemed infeasible an extra pair of partner vertices $v_{2n+1}, v_{2n+2}$ is then added to $G$ with weights $w(v_{2n+1}) = w(v_{2n+2}) = \tau$, together with a blue edge $\{v_{2n+1}, v_{2n+2}\}$. All blue edges between partners are contained in an edge set $B$ on $G$. It is useful to denote the partner of a vertex $v_i$ as $p(v_i)$; thus the set $B$ can be written as $\{\{v_i, p(v_i)\} : v_i \in V\}$. Note that $|B| = n+1$, and so $B$ is a perfect matching in all cases. 

Next, a second set of ``red'' edges, $R$, is added to $G$, which contains edges between vertices that are not partners and whose combined weight equals or exceeds $\tau$. Figure~\ref{fig:threshold} illustrates the resulting graph $G = (V, B \cup R)$ produced from the example instance $\mathcal{M}$ of the COP provided above. The graph has a noticeable pattern, with the degree of each vertex increasing in accordance with the weight of the vertices. 

Our task is to now produce a particular type of cycle on the graph $G$. Recall that a Hamiltonian cycle in a graph $G$ is a cycle that visits every vertex of $G$ exactly once. A graph containing such a cycle is said to be Hamiltonian. From this, we define a specific type of Hamiltonian cycle:

\begin{definition} % Alternating Hamiltonian Cycle
	\label{defn:althamcycle}
	Let $G = (V, B \cup R)$ be a simple, undirected graph where each edge is a member of exactly one of two sets, $B$ or $R$. $G$ contains an \emph{alternating Hamiltonian cycle} if there exists a Hamiltonian cycle whose successive edges alternate between sets $B$ and $R$.
\end{definition}

\noindent Observe that an alternating Hamiltonian cycle in $G$ corresponds to a legal sequence of the elements in $\mathcal{M}$, as the edges in $B$ represent each pair of values in $\mathcal{M}$, and edges from $R$ depict the values that meet the vicinal sum constraint. The aim of the problem is therefore to find a suitable matching subset of edges $R' \subseteq R$ that, together with the edges in $B$, form an alternating Hamiltonian cycle in $G$. The universal vertices $v_{2n+1}$ and $v_{2n+2}$, aid the construction of the alternating Hamiltonian cycle as they are able to connect to the smallest vertices; however once a cycle has been produced these vertices and any incident edges are removed, resulting in a path corresponding to a feasible COP solution $\mathcal{T}$.

Determining whether a graph is Hamiltonian is NP-complete \citep{karp1972}, whilst the problem of actually finding a Hamiltonian cycle is NP-hard. Consequently, the alternating Hamiltonian cycle problem is also NP-hard, as it is a generalisation of the former \citep{haggkvist1977}. Despite this, due to the special structure of these graphs, we are able to determine the existence of an alternating Hamiltonian cycle in polynomial time \citep{hawa2018}. \ahc{additional vertices are universal vertices with deg $2n+1$}

\subsection{The Alternating Hamiltonian Construction (AHC) Algorithm}
\label{sub:ahc}
\noindent Our algorithm for finding an alternating Hamiltonian cycle on $G$ is the Alternating Hamiltonian Construction (AHC) algorithm. This consists of two subprocedures: one to find an initial matching $R' \subseteq R$, and another to modify $R'$ so that it contains suitable edges that form an alternating Hamiltonian cycle with the fixed edges $B$ in $G$. We now describe these two methods in detail.

The first subprocedure of AHC is the Maximum Cardinality Matching (MCM) algorithm which seeks a matching $R'$ comprising $n+1$ edges on the graph induced by the set of red edges $R$. This could be achieved via standard matching processes such as the Blossom algorithm (\cite{edmonds1965}); however due to the special structure of $G$, such a matching can also be achieved via a cheaper, more efficient method as proposed by \cite{mahadev1994} and \cite{becker2015}. 

\begin{algorithm}
\caption{\textsc{MCM}}
\begin{algorithmic}[1]
	\State $m(v_i) \gets$ \textsc{null} $\forall$ $v_i \in V$, $R' \gets \emptyset$
	\For{$i \gets 1$ \To $2n+2 : m(v_i) =$ \textsc{null}}
		\For{$j \gets 2n+2$ \To $i+1 : m(v_j) =$ \textsc{null}}
			\If{$\{v_i, v_j\} \in R$}
				\State $m(v_i) \gets v_j$, $m(v_j) \gets v_i$
%				\State $R' \gets R' \cup \{\{v_i, v_j\}\}$
				\Break
			\EndIf
		\EndFor
		\If{$m(v_i) =$ \textsc{null} $\land$ $i \neq 1 \land m(m(v_{i-1})) = v_{i-1} \land \{v_{i-1}, p(v_i)\} \in R$}
%			\State $R' \gets R' \backslash \{\{v_{i-1}, m(v_{i-1})\}\}$
			\State $m(v_i) \gets m(v_{i-1})$, $m(m(v_i)) \gets v_i$
			\State $m(v_{i-1}) \gets p(v_i)$, $m(p(v_i)) \gets v_{i-1}$
%			\State $R' \gets R' \cup \{\{v_{i-1}, p(v_i)\}\}$
%			\State $R' \gets R' \cup \{\{v_i, m(v_i)\}\}$
		\EndIf
	\EndFor
	\State then need to output set $R'$ containing all matching edges
\end{algorithmic}
\label{alg:mcm}	
\end{algorithm}

\begin{comment}
\begin{algorithm}
\caption{\textsc{MCM}}
\begin{algorithmic}[1]
	\State $R' \gets \emptyset$, $V_{\textup{used}} \gets \emptyset$
	\For{$i \gets 1$ \To $2n+2$ : $v_i \notin V_{\textup{used}}$}
		\For{$j \gets 2n+2$ \To 1 : $v_j \notin V_{\textup{used}}$}
			\If{$\{v_i, v_j\} \in R$}
				\State $V_{\textup{used}} \gets V_{\textup{used}} \cup \{v_i, v_j\}$
				\State $R' \gets R' \cup \{\{v_i, v_j\}\}$
				\Break
			\EndIf
		\EndFor
	\EndFor
\end{algorithmic}
\label{alg:mcmvused}	
\end{algorithm}	
\end{comment}

\begin{comment}
The first subprocedure of AHC is the Maximum Cardinality Matching (MCM) algorithm, which is used to produce a matching $R'$ from $R$ \citep{mahadev1994}. MCM takes each vertex $v_1, v_2,\dotsc,v_{2n+2}$ and adds to $R'$ the edge from $R$ connecting $v_i$ to the largest vertex $v_j$ that is not incident to an edge in already in $R'$. Pairs of vertices incident to edges in $R'$ are then said to be \emph{matched}. As with partners, the bijective function $m : V \to V$ associates with each vertex $v_i \in V$ its match, $m(v_i)$, thus the set can then be denoted as $R' = \{(v_i, m(v_i)): v_i \in V\}$. In the event that a vertex $v_i$ is not adjacent to any other vertex via an edge in $R$, the previous vertex $v_{i-1}$ can be rematched, provided 
\begin{enumerate*}[label={(\alph*)}]
	\item $i \neq 1$;
	\item $v_{i-1}$ has been matched; and
	\item $(v_{i-1}, p(v_i)) \in R$.
\end{enumerate*} 
Then, we simply set $m(v_i) = m(v_{i-1})$, and $m(v_{i-1}) = p(v_i)$. %\ahc{Blossom?}
\end{comment}

\noindent As seen in Algorithm~\ref{alg:mcm}, vertices are considered in turn in weight-ascending order, and are matched with an adjacent vertex in $R$ not currently in $R'$ that has the largest weight. An example outcome of this process is shown in Fig.~\ref{fig:mcm}. \ahc{need to explain swap of partners, it works because if previous vertex is matched with a vertex, then current vertex should also be able to match with the vertex because vertices are considered in weight-increasing order.}

At this point, if $R'$ does not contain $n+1$ edges then there are too few edges to form an alternating Hamiltonian cycle, thus no feasible solution can exist. Otherwise, the spanning subgraph $G'=(V, B \cup R')$ will be a 2-regular graph consisting of cyclic components $C_1,C_2,\dotsc,C_z$ (as illustrated in Fig.~\ref{fig:mps}). Clearly, if $z = 1$, then $G'$ is an alternating Hamiltonian cycle and a solution has been found; otherwise $G'$ comprises multiple cycles, so AHC must find a way of connecting these components together to form a single alternating Hamiltonian cycle.

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.33\textwidth}
		\includestandalone[width=\textwidth]{figures/threshold}
		%\vspace{0.01mm}
		\caption{$G = (V, B \cup R)$}
		\label{fig:threshold}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.33\textwidth}
		\includestandalone[width=\textwidth]{figures/matching}
		%\vspace{0.01mm}
		\caption{$G' = (V, B \cup R')$}
		\label{fig:matching}
	\end{subfigure} \hspace{7mm}
	\begin{subfigure}[h]{0.2\textwidth}
		\includestandalone[width=\textwidth]{figures/mps}
		\caption{$G'$ in planar form}
		\label{fig:mps}
	\end{subfigure}
	\caption{(a) Graph $G$ modelling example instance $\mathcal{M}$; (b) $G'$ formed using $R' \subseteq R$ produced by MCM; and (c) $G'$ in planar form, clearly showing $z = 4$ components. Here, thicker blue edges are in $B$, and thinner red edges are in $R$, with the vertices' weights stated in parentheses.}
	\label{fig:mcm}
\end{figure}

\noindent The multiple cyclic components of $G'$ can be combined into a single component by removing edges from components and adding new edges between vertices of different components. As all edges in $B$ are required in the final solution, the task involves deciding which edges from each component should be removed from $R'$ and replaced with edges from $R\backslash R'$ such that the modified edge set $R'$ forms a single alternating Hamiltonian cycle with the edges in $B$. 

For this, we use the Bridge-Cover Recognition (BCR) algorithm which creates a collection of sets $\mathcal{R}^* = \{R^*_1, R^*_2,\dots\}$ each containing suitable edges from $R'$ that, once removed from $R'$, allow for specific edges from $R\backslash R'$ to be added to $R'$ that function as bridges between different components. For convenience, we say that $\mathcal{R}^*$ \textit{covers} a component $C_j$ of $G'$ if there exists a set $R^*_i \subset \mathcal{R}^*$ that contains an edge in $C_j$. BCR aims to produce a collection $\mathcal{R}^*$ that covers all $z$ components of $G'$ as follows

Firstly, the edges in $R'$ are sorted into a list $\mathcal{L}$ such that the lower-weighted vertices of the edges are in ascending order (see Fig.~\ref{fig:bcrlist}). Then, in each iteration, BCR searches from the beginning of $\mathcal{L}$ to find two or more successive edges that meet the following conditions:
\begin{enumerate}[label={(\roman*)},itemsep=-2pt,topsep=2pt]
	\item the lower-weighted vertex of each edge is adjacent to the higher-weighted vertex of the next edge via an edge in $R\backslash R'$;\label{item:adj}
	\item each edge is in a different component of $G'$; \label{item:diffcomp} and
	\item only one edge is in a component already covered by $\mathcal{R}^*$; all other edges are in components not yet covered by $\mathcal{R}^*$.\label{item:overlap}
\end{enumerate} 
These edges form a set $R^*_i$ which BCR adds to $\mathcal{R}^*$ before continuing the search for another set of edges.\footnote{When searching for edges to produce the first set, $R^*_1$, only conditions \ref{item:adj} and \ref{item:diffcomp} are required as $\mathcal{R}^* = \emptyset$.} Once the penultimate edge in $\mathcal{L}$ has been assessed, edges in $\mathcal{R}^*$ are removed from $\mathcal{L}$ and the next iteration begins. BCR ends the search successfully once $\mathcal{R}^*$ covers all $z$ components of $G'$. On the other hand, if no new sets are created during an iteration, or fewer than two edges remain in $\mathcal{L}$ after an iteration and $\mathcal{R}^*$ does not cover all $z$ components, then no more sets exist and it can be said with absolute certainty that no feasible solution exists for the given instance of the COP. Fig.~\ref{fig:bcr} shows the BCR process on our example instance, where the sets $R^*_1 = \{(v_2, v_{17}),(v_3, v_{16}), (v_4, v_{15})\}$ and $R^*_2 = \{(v_7, v_{12}), (v_8, v_{11})\}$ have been formed. As $\mathcal{R}^* =\{R^*_1, R^*_2\}$ covers all four components of $G'$ no more sets are required.

If a feasible collection $\mathcal{R}^*$ has been produced, BCR then uses each set $R^*_i \subset \mathcal{R}^*$ to procure edges from $R\backslash R'$ as follows: for each edge in $R^*_i$ in turn, the edge from $R \backslash R'$ connecting the lower-weighted vertex of the edge to the higher-weighted vertex of the next edge is added to $R'$. These edges form bridges between vertices of different components (as shown in Fig.~\ref{fig:mpsconnect}). The edges in $\mathcal{R}^*$ are then removed from $R'$, so that $|R'| = n+1$. This modified matching $R'$ is able to form an alternating Hamiltonian cycle in $G'$ with the edge set $B$. Removing the universal vertices yields an alternating Hamiltonian path which corresponds to a feasible solution $\mathcal{T}$ (Fig.~\ref{fig:solutionpath}).

\begin{comment}
\noindent Recall that an edge in a graph is a \emph{bridge} if removing the edge increases the number of components of the graph, thus the addition of a bridge decreases the number of components. To combine the components of $G'$ into a single component AHC calls upon a second subprocedure, the Bridge-Cover Recognition (BCR) algorithm, an iterative procedure that forms a collection of sets $\mathcal{R}^* = \{R^*_1, R^*_2, \dots\}$ containing specific edges from $R'$. These edges will be replaced by new edges from $R\backslash R'$ that can function as bridges between the different components. $\mathcal{R}^*$ is said to \emph{cover} a component $C_j$ if there exists a set $R^*_i \subset \mathcal{R}^*$ that contains an edge in $C_j$. BCR aims to create a collection $\mathcal{R}^*$ that covers all $z$ components of $G'$.


Firstly, the edges in $R'$ are sorted into a list $\mathcal{L}$ such that the smaller vertices of the edges are in increasing order and the larger vertices are in decreasing order, and any edges that cannot be sorted in this manner are removed from the list. An example of this list can be seen in Fig.~\ref{fig:bcrlist}. During each iteration, BCR searches from the beginning of $\mathcal{L}$ to find successive edges that meet the following conditions:
\begin{enumerate}[label={(\roman*)},itemsep=-2pt,topsep=2pt]
	\item the smaller vertex of each edge is adjacent to the larger vertex of the next edge in $\mathcal{L}$;\label{item:adj1}
	\item each edge is in a different component of $G'$; \label{item:diffcomp1} and
	\item only one of the edges is in a component already covered by $\mathcal{R}^*$, and all other edges are in components not yet covered by $\mathcal{R}^*$.\label{item:overlap1}
\end{enumerate} 
These edges form a set $R^*_i$ which BCR adds to $\mathcal{R}^*$ before continuing through the list in search of another suitable set of edges.\footnote{When searching for edges to produce the first set, $R^*_1$, only conditions \ref{item:adj} and \ref{item:diffcomp} are required as $\mathcal{R}^* = \emptyset$.} After the penultimate edge in the list has been assessed, edges in $\mathcal{R}^*$ are removed from $\mathcal{L}$ and the next iteration begins. Once $\mathcal{R}^*$ covers all components of $G'$, BCR ends the search and proceeds to obtain the bridges using the sets in $\mathcal{R}^*$. If no new sets are created during an iteration or fewer than two edges remain in the list after an iteration and $\mathcal{R}^*$ does not cover all components, then no more sets exist and BCR terminates. In these cases, it can be said with absolute certainty that no feasible solution exists for the given instance $\mathcal{M}$ of the COP.

BCR procures the bridges from a collection $\mathcal{R}^*$ covering all components of $G'$ by operating on each set $R^*_i \subset \mathcal{R}^*$ as follows: for each edge in $R^*_i$ in turn, the edge from $R \backslash R'$ connecting the smaller vertex of the edge to the larger vertex of the next edge is added to $R'$. These edges are bridges, connecting the different components together. The edges in $\mathcal{R}^*$ are then removed from $R'$, so that $|R'| = n+1$. Figure~\ref{fig:bcr} shows how BCR performs on our example instance, where the sets $R^*_1 = \{(v_2, v_{17}),(v_3, v_{16}), (v_4, v_{15})\}$ and $R^*_2 = \{(v_7, v_{12}), (v_8, v_{11})\}$ of successive edges in $\mathcal{L}$ that meet the required conditions have been formed. As $\mathcal{R}^* =\{R^*_1, R^*_2\}$ covers all four components of $G'$ no more sets are required, and BCR uses $\mathcal{R}^*$ to acquire the bridges from $R\backslash R'$, as demonstrated in Fig.~\ref{fig:bcrlist}. These bridges are added to the set $R'$, linking the components of $G'$, and replace the edges in $R'$ that appear in $\mathcal{R}^*$, shown in Figs.~\ref{fig:mpsconnect}--\ref{fig:mpscycle}. This modified matching $R'$ is able to form an alternating Hamiltonian cycle in $G'$ with the edge set $B$. Removing the universal vertices yields an alternating Hamiltonian path which corresponds to a feasible solution $\mathcal{T}$ (Fig.~\ref{fig:solutionpath}).
\end{comment}

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bcr}
		\caption{}
		\label{fig:bcrlist}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpsconnect}
		\caption{}
		\label{fig:mpsconnect}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpscycle}
		\caption{}
		\label{fig:mpscycle}
	\end{subfigure}
	\begin{subfigure}[h]{0.75\textwidth}
		\includestandalone[width=\textwidth]{figures/solutionpath}
		\caption{}
		\label{fig:solutionpath}
	\end{subfigure}
	\caption{BCR creates a collection $\mathcal{R}^* = \{R^*_1, R^*_2\}$ of edges in $R'$ that when replaced by bridges from $R\backslash R'$ connects the components of $G'$ into a single alternating Hamiltonian cycle. Dashed green edges and dotted orange edges are the bridges from $R^*_1$ and $R^*_2$ respectively. The resulting alternating Hamiltonian path corresponds to a solution $\mathcal{T}$.}
	\label{fig:bcr}
\end{figure}

\noindent In the first algorithm \citep{becker2010}, a procedure is used that searches through $\mathcal{L}$ just once to find edges sets for the collection $\mathcal{R}^*$. For some instances, although $\mathcal{R}^*$ covers all components of $G'$, the components are unable to be connected into a single alternating Hamiltonian cycle. The issue stems from the requirements for edges to form a set, where in this procedure condition~\ref{item:overlap} allows edges to be in multiple components already covered by $\mathcal{R}^*$. This causes additional bridges to be added between components that have already been connected, resulting in multiple components being formed. By restricting condition~\ref{item:overlap} such that only \emph{one} edge can be in a component that $\mathcal{R}^*$ covers, we prevent unecessary edges being added to $G'$ and ensure that components are linked to produce a single cycle. Figure~\ref{fig:overlaperror} shows the formation of $\mathcal{R}^*$ using the original procedure, where the set $R^*_2$ contains edges in \emph{two} components that $\mathcal{R}^*$ already covers. Although $\mathcal{R}^* = \{R^*_1, R^*_2\}$ covers all components of $G'$, the bridges obtained from these sets link $C_2$ and $C_3$ twice, thus connecting the four components into two different components. An additional procedure is implemented by \citet{hawa2018} that recitifies this issue, but we found that the combination of both procedures was unnecessary. Therefore, we replaced the two procedures with a single algorithm, BCR, that produces the same results in a more efficient manner. 

\begin{figure}[H]
	\centering	
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bcrerror}
		\caption{}
		\label{fig:bcrerror}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpsconnecterror}
		\caption{}
		\label{fig:mpsconnecterror}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpscycleerror}
		\caption{}
		\label{fig:mpscycleerror}
	\end{subfigure}
	\caption{Issue caused using the original procedure to find suitable edge sets, where the collection $\mathcal{R}^*$ covers all components of $G'$, but the bridges obtained from the edge sets in $\mathcal{R}^*$ form two components, as opposed to a single alternating Hamiltonian cycle.}	
	\label{fig:overlaperror}
\end{figure}

\begin{theorem}
	\label{thm:ahc}
	Let $G=(V, B \cup R)$ be a graph modelled from an instance $\mathcal{M}$ of cardinality $n$ of the COP. Then, AHC terminates in at most $O(n^2)$ time.
\end{theorem}

\begin{proof}
	The first subprocedure, MCM, produces an initial matching $R' \subseteq R$ in at most $O(n \lg n)$ time due to the sorting of the vertices. Forming the list $\mathcal{L}$ in the second subprocedure, BCR, also requires $O(n \lg n)$ time. Since each set $R^*_i$ must contain at least two edges, and $\mathcal{L}$ initial consists of up to $n+1$ edges, BCR can form up to $\frac{n+1}{2}$ sets. As $G'$ comprises a maximum of $\frac{n+1}{2}$ components, it follows that the number of edge in $\mathcal{R}^*$ required to cover all components of $G'$  is bounded by $\frac{n+1}{2}-1$. Removing edges from $\mathcal{L}$ can be performed in linear time. At least one set $R^*_i$ is created in each iteration of BCR, therefore producing the collection $\mathcal{R}^*$ is of quadratic complexity $O(n^2)$. Up to $n+1$ edges in $R'$ can be replaced by edges from $R\backslash R'$, and so can be executed in $O(n)$ time. Consequently, AHC has an overall worst case complexity of $O(n^2)$.
\end{proof}	

%--------------------------------------------------------------------------------------
\section{Heuristics for the SCPP}
\label{sec:heur}

\noindent In this section we will consider the multi-bin version of the SubSCP, the Score-Constrained Packing Problem (SCPP), in which a set $\mathcal{I}$ of $n$ items are to be partitioned into a set of bins $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ according to constraints ~\eqref{eqn:allpacked}--\eqref{eqn:feasible}. Here, a bin $S_j \in \mathcal{F}$ if and only if the total width of items in the bin, $A(S_j) = \sum_{i \in S_j} w_i$ does not exceed the bin's capacity $W$, and the vicinal sum constraint \eqref{eqn:vsc} is fulfilled. An optimal solution for the SCPP is a solution consisting of the fewest number of bins required to feasibly pack all items in $\mathcal{I}$, thus the aim is to minimise $k$.

\begin{comment}
\noindent A feasible solution for an instance of the SCPP is represented by the set $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{j=1}^{k} S_j &= \mathcal{I}, & \label{eqn:packall}\\[4pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1,2,\dotsc,k\}, \hspace{1mm} i \neq j, \label{eqn:nooverlap} \\[4pt]
	A(S_j) = \sum\nolimits_{i=1}^{|S_j|}w_i &\leq W &\quad &\forall \hspace{1mm} S_j \in \mathcal{S}, \label{eqn:capacity} \\[4pt]
	\textup{\textbf{r}}(i) + \textup{\textbf{l}}(i+1) &\geq \tau &\quad &\forall \hspace{1mm} i \in \{1, 2,\dotsc,|S_j|-1\}, \hspace{1mm} \forall \hspace{1mm} S_j \in \mathcal{S}. \label{eqn:vscbin}
	\end{alignat}
\end{subequations}

\noindent An optimal solution for the SCPP is a solution consisting of the fewest number of bins required to feasibly pack all items in $\mathcal{I}$, thus the aim is to minimise $k$. Here, a bin $S_j \in \mathcal{F}$ if the total width of items in the bin does not exceed the bin's capacity \eqref{eqn:capacity} and the vicinal sum constraint is fulfilled \eqref{eqn:vscbin}.
\end{comment}

The BPP is known to be NP-hard \citep{garey1979}, and since the SCPP generalises the BPP it follows that the SCPP is also NP-hard. Assuming $P \neq NP$, we cannot hope to find an optimal solution for all instances of the SCPP in polynomial time. One approach for such problems are heuristics, which trade optimality for speed. 

A well-known heuristic for the BPP is First-Fit (FF), a greedy online algorithm that packs each item one by one in some given order into the lowest-indexed bin such that the capacity of the bin is not exceed, opening a new bin when required. It is known that there always exists at least one ordering of the items such that FF produces an optimal solution \citep{lewis2009}, though identifying such an ordering is itself NP-hard. An improvement on FF is the First-Fit Decreasing (FFD) heuristic, which initially sorts the items in non-increasing order of size. In 2007, it was proven that the worst case for FFD is $\frac{11}{9}k + \frac{6}{9}$, and that this bound is tight \citep{dosa2007}. Due to the initial sorting of the items, the time complexity of FFD is $O(n \lg n)$. Similar heuristics include Best-Fit (BF), in which each item is packed into the fullest bin that can accommodate the item without being overfilled, and its offline counterpart Best-Fit Decreasing (BFD). A comprehensive overview of these heuristics and related methods can be seen in \citet{coffman1984}. More advanced heuristics for the BPP have been developed with positive results, such as the Minimum Bin Slack (MBS) heuristic \citep{gupta1999}, which focuses on packing each bin in turn rather than each item, and modifications of MBS such as the Perturbation-MBS' heuristic of \citet{fleszar2002}. 

For the BPP, a basic lower bound for $k$ is the theoretical minimum, $t = \ceil{\sum_{i=1}^{n} w_i / W}$ \citep{martello1990l}, however $t$ will not perform as accurately for the SCPP as it fails to account for the vicinal sum constraint. For example, given a set of $n$ items in which the largest score width $b_i < \tau / 2$, it is clear that no pairs of score widths can fulfil the vicinal sum constraint and so each item must be packed into individual bins, thus $|\mathcal{S}| = n$.

The vicinal sum constraint also introduces further differences in solutions for the BPP and SCPP. The obvious disparity is that of the ordering and orientation of the items in the bins: unimportant in the BPP, but vital for the feasibility of a solution for the SCPP. Another distinction arises when attempting to modify solutions. In the BPP, a bin remains feasible when an item is removed or a new item is added (provided the bin can accommodate the item), whereas for the SCPP this may render a bin infeasible, as the new adjacent score widths may not abide by the vicinal sum constraint. Consequently, heuristics for the BPP will need to be adapted in order to produce feasible solutions for the SCPP.

\begin{comment}
A solution for the BPP remains feasible when an item is removed or a new item is added to a bin (provided the bin can accommodate said item), whereas this may render a solution for the SCPP infeasible, as the new adjacent score widths may not abide by the vicinal sum constraint. As a result of these differences, basic heuristics designed for the BPP cannot be used for the SCPP as there is no guarantee that the final solution will be feasible.  An example of this issue is provided in Fig.~\ref{fig:ffd} where a solution for an instance of the SCPP has been generated using FFD. The heuristic's inablity to rotate or reorder items produces alignments of items that violate the vicinal sum constraint in three of the bins, thus resulting in an entirely infeasible solution. Notice however that heuritics for the BPP can still produce feasible, albeit not optimal, solutions for the TPP and CSP-SDCL, as the only hard constraint in these problems is the requirement that no bin is overfilled~\eqref{eqn:capacity}.

\begin{figure}[h]	
	\centering
	\includestandalone[width=0.35\textwidth]{figures/ffd}
	\caption{An infeasible solution produced using FFD for an example instance of the SCPP. Here, $|\mathcal{I}| = 15$, $W = 1000$, $\tau = 70$, and the theoretical minimum $t = 6$.}	
	\label{fig:ffd}
\end{figure}
\end{comment}

As the SCPP is a relatively new problem, few methods have been seen in literature. Some basic heuristics were introduced in \citet{hawa2018}, two of which are based on the FFD heuristic for the BPP. The first, named the Modified First-Fit Decreasing (MFFD) heuristic, performs in the same fashion as FFD with the additional condition that an item $i$ can only be packed into a bin $S_j$ if the score width on the end of $S_j$ and one of the score widths $a_i$ or $b_i$ meet the vicinal sum constraint. An improvement on MFFD is their second heuristic, MFFD$^+$, which incorporates AHC. Rather than attempting to pack each item into the ends of bins, MFFD$^+$ calls upon AHC to find a feasible ordering of all items currently in a given bin \emph{and} the item $i$ to be packed, i.e. AHC is used to determine the membership of $\mathcal{F}$.\footnote{If an item $i$ is the first to be packed into a bin, it is placed in a regular orientation, $(a_i, b_i)$.} Clearly MFFD$^+$ is the superior of the two, as the application of AHC guarantees that a feasible configuration of items in a bin will be found if it exists. The restriction on MFFD of only packing items in the ends of bins has the potential to increase the number of bins being used in a final solution unnecessarily. 

\begin{comment}
Figure~\ref{fig:mffdvsmffdplus} compares solutions produced by MFFD and MFFD$^+$ using the same instance of the SCPP as seen in Fig.~\ref{fig:ffd}. Note how MFFD$^+$ is able to form a solution using the same number of bins as FFD while still satisfying the vicinal sum constraint.

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/mffd}
		\caption{MFFD}
		\label{fig:mffd}
	\end{subfigure} \hspace{15mm}
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/mffdplus}
		\caption{MFFD$^+$}
		\label{fig:mffdplus}
	\end{subfigure}
	\caption{Solutions using MFFD and MFFD$^+$ for the same instance of the SCPP as in the previous figure, where MFFD$^+$ has generated a feasible solution using the same number of bins as FFD.}
	\label{fig:mffdvsmffdplus}
\end{figure}


\subsection{Experimental Results - Heuristics}
\label{sub:expheur}
\noindent Although a comparison of these heuristics is given in \citet{hawa2018}, we performed our own tests where the MFFD$^+$ algorithm uses our updated version of the Alternating Hamiltonian Construction algorithm which includes the preliminary test and the new BCR procedure. We produced two different types of problem instances: ``artificial'', in which the items are strongly heterogeneous (the items have varying widths and score widths); and ``real'', where items are weakly hetergeneous (many items have the same dimensions). For each type, 1000 instances were generated using sets of 100, 500, and 1000 items, giving a total of 6000 problem instances. All items have widths $w_i \in [150,1000]$ and score widths $a_i, b_i \in [1,70]$ selected uniform randomly, and equal height $H=1$. For the real instances, the number of item types was chosen uniform randomly between 10 and 30, and the number of items within each type also assigned uniform randomly. Two different bin sizes, $W = 2500$ and 5000 (also of height $H=1$) were used in the experiments to alter the number of items per bin, and the minimum scoring distance $\tau$ was fixed at 70mm - the industry standard. The problem instance generator and instance files used is provided by \citet{hawa2019}. Table~\ref{table:heur} contains the results from the experiments. All individual runs were completed in under 125ms.

%\scpp{As optimal solutions are not available, we calculated the solution quality $q = |\mathcal{S}|/ t$ which compares each solution to the theoretical minimum.}

\begin{table}[h!]
\centering
\caption{\scpp{Results obtained using MFFD and MFFD$^+$ on different types of SCPP instances. Figures in bold indicate the best results for each instance type.}}
\begin{threeparttable}
\begin{tabular}{lcrcrc crcr crcr} %Type, X, |I|, X, t, X, X, |S|, X, #t, X, |S|, X, #t.
	\toprule
	& & & & & & & \multicolumn{3}{c}{MFFD} &\phantom{ab}& \multicolumn{3}{c}{MFFD$^+$}\\
	\cmidrule{8-10} \cmidrule{12-14}
	\multicolumn{1}{c}{Type, $W$} && \multicolumn{1}{c}{$n$} && \multicolumn{1}{c}{$t$\tnote{$a$}} &&& \multicolumn{1}{c}{$|\mathcal{S}|$\tnote{$b$}} && \multicolumn{1}{c}{$\# t$\tnote{$c$}} && \multicolumn{1}{c}{$|\mathcal{S}|$} && \multicolumn{1}{c}{$\# t$}\\ \midrule	
	a, $W=2500$ && 100 && 23.323 &&& 30.754 && 0 && \textbf{28.457} && \textbf{26} \\
	&& 500 && 114.942 &&& 140.206 && 0 && \textbf{132.647} && 0 \\
	&& 1000 && 229.437 &&& 271.919 && 0 && \textbf{258.388} && 0 \\
	\midrule
	a, $W=5000$ && 100 && 11.922 &&& 23.583 && 0 && \textbf{19.881} && \textbf{7} \\
	&& 500 && 57.722 &&& 103.209 && 0 && \textbf{89.544} && 0 \\
	&& 1000 && 114.965 &&& 198.325 && 0 && \textbf{172.613} && 0 \\
	\midrule
	\midrule
	r, $W=2500$ && 100 && 23.473 &&& 37.069 && 5 && \textbf{35.419} && \textbf{16} \\
	&& 500 && 115.239 &&& 184.106 && 0 && \textbf{177.249} && 0 \\
	&& 1000 && 229.946 &&& 368.453 && 0 && \textbf{355.042} && 0 \\
	\midrule
	r, $W=5000$ && 100 && 11.981 &&& 32.348 && 1 && \textbf{29.611} && \textbf{5} \\
	&& 500 && 57.865 &&& 163.819 && 0 && \textbf{153.416} && 0\\
	&& 1000 && 115.227 &&& 328.618 && 0 && \textbf{308.642} && 0 \\
	\bottomrule
\end{tabular}
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
\end{tablenotes}	
\end{threeparttable}	
\label{table:heur}
\end{table}


It can be seen that MFFD$^+$ produces solutions using fewer bins on average than MFFD for all 12 instance classes. MFFD was only able to find solutions using $t$ bins for two instance classes, whereas MFFD$^+$ produced solutions using $t$ bins for four instances classes. Solutions using fewer bins were found less frequently on average when the bin size is larger. A larger bin size means the bins can accommodate more items, however this increases the number of adjacent score widths that must fulfil the vicinal sum constraint. 

%\scpp{The solution quality $q$ for the six instance classes where $W = 5000$ is significantly higher than when $W = 2500$.}
%\scpp{Note also that $q$ is higher for real instances in comparison to artificial instances, as due to the reduced variety of item widths and score widths it is difficult to attain feasible packings.} \note{$q$ has now been removed from tables as it is redundant, can discuss in text rather than displaying in tables - change text accordingly.}

Indeed, it may be that optimal solutions were found in many of these instances, as the lower bound for $k$ may actually be higher than the theoretical minimum calculated. These experiments provide clear results that the use of an exact polynomial-time algorithm, AHC, within a heuristic is more powerful than using a simple heuristic alone. In spite of this, MFFD$^+$ lacks the capability of rearranging items \emph{between} bins, that is, once an item has been packed into a bin, it must stay in that bin (although the position of the item in the bin can change due to the use of AHC). It follows that every item packed has an effect on where the unplaced items can be placed, i.e. where an item is packed is dependent on where previous items have been packed. These limitations associated with greedy heuristics lead us to explore other superior methods.
\end{comment}
% Notes - Experiments Heuristics
\begin{comment}
{\color{myPink}
\begin{itemize}[leftmargin=*]
	\item Number of types on average real instances.
	\item Number of items per bin.
\end{itemize}
}
\end{comment}

%--------------------------------------------------------------------------------------
\section{An Evolutionary Algorithm for the SCPP}
\label{sec:ea}
\noindent We now introduce an evolutionary algorithm (EA) for the SCPP to improve on the previous heuristics. An evolutionary algorithm is a metaheuristic optimisation algorithm inspired by the natural evolutionary process. Candidate solutions to the problem form the initial population, and procedures emulating natural selection, reproduction, recombination and mutation are used to create the next generation of solutions. This iterated process results in the evolution of the population. EAs have been used for a variety of grouping problems with positive results \citep{lewis2017, falkenauer1996, quiroz2015}. Within the EA framework, we investigate three different group-based recombination operators as well as a local search procedure inspired by \citet{martello1990l}. The AHC algorithm described in Section~\ref{sec:ahc} is also integrated into the EA to solve instances of the SubSCP.

\subsection{Recombination}
\label{sub:recomb}
\noindent Recombination is used in EAs to generate new solutions by taking existing parent solutions and combining them to create one or more offspring solutions. A recombination operator determines which elements from each parent should be inherited by the offspring. The operators used for our EA use two parent solutions, $\mathcal{S}_1, \mathcal{S}_2$, to form one offspring solution $\mathcal{S}$. These operators have been designed to ensure the offspring is feasible, and so this can result in a partial offspring solution. In such cases, a repair procedure described below is used to build a full offspring solution $\mathcal{S}$.


The first operator is based on the grouping genetic algorithm (GGA) of \citet{falkenauer1992}. Starting with an empty offspring $\mathcal{S} = \emptyset$, the bins of the second parent solution $\mathcal{S}_2$ are randomly permuted and two bins $S_i$ and $S_j$ are selected randomly (where $1 \leq i < j \leq |\mathcal{S}_2|$). All bins between and including $S_i$ and $S_j$ are then copied into the offspring solution $\mathcal{S}$. GGA then adds to the offspring all bins from $\mathcal{S}_1$ that do not contain items already present in the offspring.

%Note that the bins chosen from $\mathcal{S}_2$ cannot both be the outermost bins, that is, GGA cannot choose both $i = 1$ \emph{and} $j = |\mathcal{S}_2|$, as doing so would result in all bins from $\mathcal{S}_2$ being copied into the offspring, preventing the addition of any bins from $\mathcal{S}_1$.

The second operator we implemented is the alternating grouping crossover (AGX), and is analogous to that of \citet{quiroz2015} proposed for the BPP. Starting with the parent solution containing the fullest bin, AGX inserts this bin into an offspring solution $\mathcal{S}$, and bins containing items in $\mathcal{S}$ are removed from the other parent.\footnote{For both AGX and AGX', in the event that both parents contain bins with equal maximum fullness/number of items, the initial parent solution is chosen at random.} The operator then inserts the fullest bin from the modified parent into $\mathcal{S}$ and removes bins from the first parent. AGX continues to alternate between parents, selecting the fullest bin $max_{S_j \in \mathcal{S}_p} (A(S_j))$ (where $\mathcal{S}_p$ is the parent solution under consideration, $p \in \{1,2\}$) until at most $min (|\mathcal{S}_1|,|\mathcal{S}_2|) - 1$ bins have been added to the offspring solution.

Our final operator, AGX$'$, performs in a similar manner to AGX, however rather than choosing the fullest bin to insert into the offspring solution AGX$'$ selects bins containing the most items, $max_{S_j \in \mathcal{S}_p} (|S_j|)$. This method has the ability to preserve bins containing items that are harder to pack along with other items. 

In order to maintain feasibility, the operators remove entire bins containing duplicate items rather than individual items. These bins may also contain items that are not present in the offspring solution. Consequently, on completion of the crossover, the offspring solution $\mathcal{S}$ may not contain all $n$ items and is therefore not yet a full solution. To rectify this, MFFD$^+$ is applied using the missing items to form a partial solution $\mathcal{S}'$. The partial solutions $\mathcal{S}$ and $\mathcal{S}'$ are then used as input into a local search procedure to create a full feasible offspring solution. Figure~\ref{fig:recomb} shows the offspring solution $\mathcal{S}$ produced from two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, using each of the recombination operators, along with the individual items missing from each offspring.

\begin{figure}[H]	
	\centering
	\begin{minipage}{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/parentS1}
	\end{minipage} \hspace{15mm}
	\begin{minipage}{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/parentS2}
	\end{minipage}
\end{figure}

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/gga}
		\caption{GGA}
		\label{fig:gga}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/agx}
		\caption{AGX}
		\label{fig:agx}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/agxdash}
		\caption{AGX$'$}
		\label{fig:agxdash}
	\end{subfigure}
	\caption{Partial offspring solutions and missing items created from parent solutions $\mathcal{S}_1$ and $\mathcal{S}_2$ using different recombination operators. (a) Bins $S_2, S_3$ and $S_4$ are added to the offspring from $\mathcal{S}_2$; (b)--(c) $\mathcal{S}_1$ is the initial parent for both AGX and AGX$'$ as it contains both the fullest bin and the bin with the most items.}
	\label{fig:recomb}
\end{figure}

\subsection{Local Search}
\label{sub:localsearch}
\noindent Our local search method takes two subsolutions, $\mathcal{S}$ and $\mathcal{S}'$, containing bins that together form a full solution containing all items in $\mathcal{I}$, and shuffles items between the bins of the each subsolution. By moving larger items into $\mathcal{S}$, the fullness $A(S_j)$ of the bins $S_j \in \mathcal{S}$ increases whilst the number of items per bin is maintained or decreases. Simultaneously, items moved into $\mathcal{S}'$ are smaller and therefore easier to repack into bins in $\mathcal{S}$. The procedure begins by permuting the bins $\mathcal{S}$ and $\mathcal{S}'$, before attempting to move items in four stages:
\begin{enumerate}[label={(\arabic*)},itemsep=-2pt,topsep=2pt]
	\item swapping a pair of items from a bin in $\mathcal{S}$ with a pair of items from a bin in $\mathcal{S}'$;\label{item:pairpair}
	\item swapping a pair of items from a bin in $\mathcal{S}$ with an individual item from a bin in $\mathcal{S}'$;\label{item:pairsin}
	\item swapping individual items from bins in $\mathcal{S}$ and $\mathcal{S}'$;\label{item:sinsin} and
	\item moving an item from a bin in $\mathcal{S}'$ to a bin in $\mathcal{S}$.\label{item:movesin}
\end{enumerate} 
During stages \ref{item:pairpair}--\ref{item:sinsin}, the width of the item(s) from $\mathcal{S}'$ must exceed the width of the item(s) from $\mathcal{S}$. Once a swap or move has been perfomed, the procedure immediately moves on to the next stage. This method is repeated until all four stages have been executed in succession with no changes to $\mathcal{S}$ or $\mathcal{S}'$. Then, MFFD$^+$ is applied to any items remaining in $\mathcal{S}'$, generating a new feasible partial solution $\mathcal{S}''$, with $|\mathcal{S}''| \leq |\mathcal{S}'|$. The bins in $\mathcal{S}'$ are then inserted into $\mathcal{S}$ to form a full feasible solution. \ea{a swap/move is only performed if vsc met, AHC run every time.}

This method is based on the dominance criterion of \citet{martello1990l}: if a bin $S_x$ \emph{dominates} a bin $S_y$, then a solution containing $S_x$ will have no more bins than a solution containing $S_y$. Variations of this method can be seen in \citet{lewis2009, lewis2017, falkenauer1996, levine2004}, however the addition of the vicinal sum constraint results in fewer changes than seen in these previous implementations. By iterating the stages numerous distinct subsets of items in the bins are produced, generating more possibilities for feasible orderings of items. It can be seen that this procedure cannot increase the number of bins in a solution, and also has the ability to decrease the number of bins.


\subsection{Evolutionary Algorithm Framework}
\label{sub:eaframework}
\noindent Our EA for the SCPP begins by producing candidate solutions to form an initial population, with one solution created using MFFD$^+$ and the rest using MFFR$^+$ (the same method as MFFD$^+$ with the items in a random order). Each solution is mutated before being inserted into the population. The mutation of a solution $\mathcal{S}$ consists of permuting the bins and inserting $1 < r < |\mathcal{S}|$ bins selected randomly from $\mathcal{S}$ into a set $\mathcal{S}'$, and then executing local search on these two partial solutions to produce a full feasible solution $\mathcal{S}$. 

Each iteration of the EA involves selecting two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, from the population at random, applying a recombination operator to produce an offspring solution $\mathcal{S}$, then finally mutating $\mathcal{S}$ before replacing the least fit of the two parents in the population.

For the classical one-dimensional BPP, a tailored function can be used to determine the fitness of a solution, as opposed to simply relying on the number of bins within a solution. The reason for this is two-fold: firstly, it is difficult to distinguish between solutions as many of them will have the same number of bins, and secondly, we note that the fitness of a solution not only depends on the number of bins used, but also \emph{how} the items are packed into the bins. It is clear that if the bins' capacities are taken advantage of, fewer bins will be needed to pack all items compared to if the bins are only half full, requiring more bins than necessary to accommodate all of the items. A solution comprising fuller bins may also contain bins that are nearly empty, which is beneficial as it allows futher items to be packed, or the residual material could be used for other means.

\citet{falkenauer1992} make use of the following function to calculate the fitness of a solution $\mathcal{S}$ of the BPP:
\begin{equation}
f(\mathcal{S}) = \frac{\sum_{S_j \in \mathcal{S}} (A(S_j)/W)^2}{|\mathcal{S}|}
\label{eqn:fitness}
\end{equation}

\noindent which assigns higher values to fitter solutions. Here, if $|\mathcal{S}_1| < |\mathcal{S}_2|$  then $f(\mathcal{S}_1) > f(\mathcal{S}_2)$, hence a global optimum for this fitness function is associated with the optimal solution containing the fewest number of bins. Note that this is dependent on the characteristic of the BPP where at most one bin in a solution is less or equal to half full, as the contents of two less than half-full bins can be combined into a single bin. However, this is not the case for the SCPP, as the items of two half-full bins may not be able to form a single feasible packing due to the vicinal sum constraint, hence solutions for the SCPP may contain multiple bins $S_j$ such that $A(S_j) \leq W/2$. As a result, we have observed a number of cases of the SCPP where although  $|\mathcal{S}_1| < |\mathcal{S}_2|$, the corresponding fitness values $f(\mathcal{S}_1) < f(\mathcal{S}_2)$. Therefore, we cannot rely on the fitness function alone to guide us towards an optimal solution, as this may lead to a final solution comprising more bins than necessary. Instead, we use the number of bins to determine the quality of a solution, only using the fitness function when comparing solutions of containing equal number of bins to select the solution with more efficient packing.


% Notes - EA
\begin{comment}
{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item Fitness function by Falkenauer
	\item If $|\mathcal{S}_1| < |\mathcal{S}_2|$  then $f(\mathcal{S}_1) > f(\mathcal{S}_2)$, hence a global optimum for this fitness function is associated with the optimal solution containing the fewest number of bins.
	\item This is using the characteristic of the classical 1D BPP, where at most one bin in a solution is less than half full, as the contents of two half-full bins can be combined into one.
	\item However, solutions to the SCPP can contain multiple bins that are less than half-full, as the items within these bins may not be able to form a feasible alignment/packing due to the vsc.
	\item Many instances where although $|\mathcal{S}_1| < |\mathcal{S}_2|$, we have $f(\mathcal{S}_1) < f(\mathcal{S}_2)$.
	\item Thus we cannot rely on the fitness functionalone to guide us towards an optimal solution.
	\item Instead, we use the number of bins to compare the solutions, only using the fitness function when comparing two solutions of the same size to determine which solution has the better packing.
	\item Iterative procedure, plus local search and xover, swapping items between bins, better than previous heuristics as repetition means more possible item combinations in bins, EA has the ability to move items between bins.
	\item EA consists of multiple iterations, previous heuristics only one.
	%\item What is GGA's aim?
	%\item How is AGX useful? Choose fullest bin, less waste, less bins required.
\end{itemize}
}
\end{comment}

\subsection{Experimental Results}
\label{sub:expea}
\noindent Our EA was executed on a set of problem instances for the SCPP created using a problem instance generator available online \citep{hawa2019}. The set contains two types of problem instances: ``artificial'', in which the items are strongly heterogeneous (the items have varying widths and score widths); and ``real'', where items are weakly hetergeneous (many items have the same dimensions). Each type contains three subsets of 1000 instances for 100, 500, and 1000 items, giving a total of 6000 problem instances. All items have widths $w_i \in [150,1000]$ and score widths $a_i, b_i \in [1,70]$ selected uniform randomly, and equal height $H=1$. For the real instances, the number of item groups was chosen uniform randomly between 10 and 30, and the number of items within each group also assigned uniform randomly.

For our experiments, we used two different bin sizes, $W = 2500$ and 5000 (also of height $H=1$) were used in the experiments to alter the number of items per bin, and the minimum scoring distance $\tau$ was set to 70mm - the industry standard. After preliminary experiments, we settled for an initial population containing 25 candidate solutions. Across all instances, the EA had a fixed time limit of 600 seconds. The MFFD$^+$ heuristic descibed in Section~\ref{sec:heur} was also executed on the same set of problem instances to provide comparative results. Table~\ref{table:ea} displays the results obtained from the experiments using the different recombination operators and bin sizes.

%\noindent To ensure a fair comparison with the heuristics in Section~\ref{sec:heur}, the same set of problem instances generated for the previous experiments were used to test our EA. After preliminary experiments, we settled for an initial population containing 25 candidate solutions. Across all instances, the EA had a fixed time limit of 600 seconds. Table~\ref{table:ea} displays the results obtained from the EA experiments using the different recombination operators and bin sizes.


\begin{table}[h!]
\centering
\caption{\ea{Results obtained from EA using different recombination operators. Figures in bold indicate the best results for each instance type.}}
\small
\begin{threeparttable}
\begin{tabular}{crr c rr c rr c rr c rr}
	\toprule
	& & & & \multicolumn{2}{c}{GGA} &\phantom{a}& \multicolumn{2}{c}{AGX} &\phantom{a'}& \multicolumn{2}{c}{AGX$'$} &\phantom{a'}& \multicolumn{2}{c}{MFFD$^+$}\\
	\cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12} \cmidrule{14-15}
	\multicolumn{1}{c}{Type, $W$} & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$t$\tnote{$a$}} && \multicolumn{1}{c}{$|\mathcal{S}|$\tnote{$b$}} & \multicolumn{1}{c}{$\# t$\tnote{$c$}} && \multicolumn{1}{c}{$|\mathcal{S}|$} & \multicolumn{1}{c}{$\# t$} && \multicolumn{1}{c}{$|\mathcal{S}|$} & \multicolumn{1}{c}{$\# t$} && \multicolumn{1}{c}{$|\mathcal{S}|$} & \multicolumn{1}{c}{$\# t$}\\
	\midrule
	a, 2500 & 100 & 23.323 && 23.358 & 973 && \textbf{23.343} & \textbf{985} && 23.355 & 975 && 28.457 & 26\\
	& 500 & 114.942 && 116.44 & 316 && 116.711 & 280 && \textbf{116.326} &\textbf{347} && 132.647 & 0\\
	& 1000 & 229.437 && 234.482 & 14 && 235.029 & 17 && \textbf{233.988} &\textbf{21} && 258.388 & 0\\
	\midrule
	a, 5000 & 100 & 11.922 && \textbf{12.265} & \textbf{849} && 12.315 & 825 && 12.304 & 832 && 19.881 & 7\\
	& 500 & 57.722 && \textbf{61.836} & \textbf{65} && 62.322 & 46 && 62.546 & 51 && 89.544 & 0\\
	& 1000 & 114.965 && \textbf{126.308} & 0 && 126.918 & 0 && 127.122 & 0 && 172.613 & 0\\
	\midrule
	\midrule
	r, 2500 & 100 & 23.473 && 26.012 & \textbf{569} && 26.084 & 568 && \textbf{25.955} &\textbf{569} && 35.419 & 16\\
	& 500 & 115.239 && 134.342 & 45 && 134.586 & \textbf{55} && \textbf{134.24} & 39 && 177.249 & 0\\
	& 1000 & 229.946 && \textbf{270.396} & 1 && 270.654 & \textbf{2} && 270.59 & 0 && 355.042 & 0\\
	\midrule
	r, 5000 & 100 & 11.981 && \textbf{17.571} & \textbf{484} && 17.621 & 466 && 17.608 & 466 && 29.611 & 5\\
	& 500 & 57.865 && \textbf{93.875} & 85 && 94.549 & \textbf{86} && 94.186 & 82 && 153.416 & 0\\
	& 1000 & 115.227 && \textbf{193.08} & \textbf{29} && 193.658 & 27 && 193.578 &\textbf{29} && 308.642 & 0\\
	\bottomrule
\end{tabular}	
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
\end{tablenotes}
\end{threeparttable}
\label{table:ea}
\end{table}


Firstly, it is immediately evident that the EA outperforms the previous heuristics, producing solutions using $t$ bins in all but one of the 12 subclasses. Fewer iterations of the EA were performed as $n$ and $W$ increased, with the average number of iterations ranging from 300,000 to 1400 when $W=2500$ for artificial instances using 100 and 1000 items respectively. The corresponding figures for real instances when $W=5000$ were 470 and 65 iterations.

It can be seen that the EAs using the GGA and AGX$'$ recombination operators generate the best solutions overall. GGA consistently produces the best results for both artificial and real instance types when the larger bin size is used, where higher quality solutions contain 8.6 items per bin on average. The random selection of bins from parents results in entire solutions being evaluated, rather than individual bins, thus good combinations of bins can be found to form better solutions.

\noindent On the other hand, when the smaller bin capacity $W=2500$ is used, where the best solutions average 4.3 items per bin, AGX$'$ can be see to produce the more optimal solutions. By selecting bins containing the most items, fewer bins will be needed to pack all of the items, and bins containing items that are considered harder to pack are preserved throughout the EA. 

AGX generates the least favourable results, particularly for the real instance types, where it is outperformed by GGA and AGX$'$ in all 6 sub classes in regards to the average number of bins per solution $|\mathcal{S}|$. Selecting bins based on fullness may mean choosing bins that contain fewer items, thus extra bins are required to pack all items. However, note that AGX yields the highest number of solutions containing $t$ bins in three of the subclasses, suggesting a high variance in the quality of solutions produced.

\begin{comment}
{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item Better than heuristics - compare.
	\item Artificial
	\begin{itemize}
		\item AGX$'$ best for $W=2500$, GGA best for $W=5000$ -- what does this show?
		\item NSA105, no recombination operator produced a single solution where $|\mathcal{S}| = t$.
		\item As $n$ increases, difference between $t$ and average $|\mathcal{S}|$ increases, i.e. for NSA12, difference is between 0.045 -- 0.026, whereas for NSA105, difference is between 13.157 -- 12.708.
		\item Number of solutions where $|\mathcal{S}| = t$ decreases as $n$ increases.
		\item Number of solutions where $|\mathcal{S}| = t$ less when $W=5000$ than when $W=2500$ for same number of items $n$.
		\item Difference between average $|\mathcal{S}|$ and $t$ smaller when $W=2500$ than when $W=5000$ for same number of items $n$.
	\end{itemize}
	\item Real
	\begin{itemize}
		\item GGA better when $W=5000$.
	\end{itemize}	
	\item Time graph output.
	\item Number of EA iterations within time limit, how does it vary between recombination operators?
\end{itemize}
}
\end{comment}

%--------------------------------------------------------------------------------------
\section{A Hybrid Metaheuristic Approach}
\label{sec:cmsa}
\noindent In this section, we discuss a metaheuristic approach to the SCPP, combined with an exact method.

\begin{definition}
	\label{defn:exactcover}
	Let $\mathcal{S}$ be a collection of subsets of a set $\mathcal{B}$. Then a subcollection $\mathcal{S}^*$ of $\mathcal{S}$ is an exact cover if and only if each element in $\mathcal{B}$ is contained in exactly one subset in $\mathcal{S}^*$.
\end{definition}	

\noindent The exact cover problem, determining whether a subcollection $\mathcal{S}^*$ exists, is a decision problem and one of \citeauthor{karp1972}'s 21 NP-complete problems \citeyearpar{karp1972}. However, if it is known that an exact cover exists, the problem can be altered to instead find the smallest subcollection. 

Consider an $m\times n$ binary matrix $\mathcal{B}$, and let $M = \{1,2,\dotsc,m\}$ and $N = \{1,2,\dotsc,n\}$ be the rows and columns of the matrix respectively. Then, an element of the matrix $b_{ij} = 1$ if row $i$ covers column $j$. The task involves finding a minimum cardinality subset of rows $\mathcal{S}^* \subseteq M$ such that each column $j \in N$ is covered by exactly one row $i \in \mathcal{S}^*$. This problem can be formulated as the following integer linear program:

\begin{subequations}
	\begin{alignat}{3}
		\text{minimise  } &\sum_{i \in M} x_i & \label{eqn:objfn}\\[3pt]
		\text{subject to  } &\sum_{i \in M} b_{ij} x_i = 1 &\quad &\forall \hspace{1mm} j \in N \label{eqn:cover}\\[3pt]
		&x_i \in \{0,1\} & &\forall \hspace{1mm} i \in M \label{eqn:binary}
	\end{alignat}
\end{subequations}

\[x_i =
\begin{cases} 
1 & \text{if } i \in \mathcal{S}^* \\
0 & \text{otherwise} 
\end{cases}
\]

\noindent Given a set of feasible bins, it can be seen that this formulation can be used to find a solution for the SCPP, where each row $i \in M$ represents a bin, and each column $j \in N$ represents a item. Since the bins are already feasible, the complications associated with the vicinal sum constraint are eliminated.

Ideally, the entire set of feasible bins $\mathcal{F}$ would be used to form the matrix, however this could include hundred of millions of bins. Instead, we implemented the exact cover method within the Construct, Merge, Solve \& Adapt (CMSA) algorithm of \citet{blum2016}. Rather than having a population of full solutions as in our EA, CMSA operates on a set $\mathcal{B}$ containing individual feasible bins. The pseudocode for CMSA is provided in Algorithm~\ref{alg:cmsa}.

\begin{algorithm}
\caption{\textsc{CMSA} ($p$, $maxAge$)}
\begin{algorithmic}[1]
	\State $\mathcal{S}_{bsf} \gets \emptyset$, $\mathcal{B} \gets \emptyset$
	\While{time limit not reached}
		\For{$i\gets 1$ \To $p$}
			\State $\mathcal{S} \gets$ \textsc{MFFR}$^+$($\mathcal{I}$)
				\ForAll{$b \in \mathcal{S}$ \textbf{and} $b \notin \mathcal{B}$}
					\State $age[b] \gets 0$
					\State $\mathcal{B} \gets \mathcal{B} \cup \{b\}$
				\EndFor
		\EndFor
		\State $\mathcal{S}^* \gets$ \textsc{ExactCover}($\mathcal{B}$)
		\If{$\mathcal{S}^*$ \cmsa{is better than} $\mathcal{S}_{bsf}$}
			\State $\mathcal{S}_{bsf} \gets \mathcal{S}^*$
		\EndIf
		\ForAll{$b \in \mathcal{B}$}
			\If{$b \in \mathcal{S}^*$}
				\State $age[b] \gets 0$
			\ElsIf{$b \notin \mathcal{S}^*$}
				\State $age[b] \gets age[b] + 1$
				\If{$age[b] = maxAge$}
					\State $\mathcal{B} \gets \mathcal{B} - \{b\}$
				\EndIf
			\EndIf		
		\EndFor
	\EndWhile
	\Return $\mathcal{S}_{bsf}$
\end{algorithmic}
\label{alg:cmsa}	
\end{algorithm}	

\noindent Firstly, a fixed number $p$ of solutions are produced using MFFR$^+$, and the bins of each solution are inserted into a set $\mathcal{B}$, with the age of every new bin $b$, $age[b]$, set to zero (lines 3--7). The exact cover algorithm is executed, generating an optimal solution $\mathcal{S}^*$ which, if better than the best-so-far solution $\mathcal{S}_{bsf}$, is stored as the new best-so-far solution (lines 8--10). The set $\mathcal{B}$ is then adapted by resetting the age of bins used in $\mathcal{S}^*$ to zero and increasing the age of all other bins by one (lines 11--15). Finally, any bins in $\mathcal{B}$ whose age has reached the maximum, $maxAge$, are removed from the set (lines 16--17). This process is repeated until a set time limit has been reached. The adaptation stage aids the process by removing bins from $\mathcal{B}$ that have not contributed to an optimal solution for a period of time, controlling the size of $\mathcal{B}$ and thus the speed of the exact solver, and also by retaining bins in $\mathcal{B}$ that haven been used in optimal solutions, which could prove to be useful in subsequent iterations of CMSA.

One method of solving the exact cover problem is by using a recursive depth-first backtracking process, referred to as ``Algorithm X'' by Donald \citet{knuth2000}, which is implemented using the ``dancing links'' technique to find all solutions to a given problem. As we are only interested in the solution with the fewest bins, we modified Algorithm X to only search for solutions that improve upon the best solution found so far.

\subsection{Experimental Results - CMSA}
\label{sub:expcmsa}
\noindent Table~\ref{table:cmsa} compares the performance of the CMSA and EA algorithms on 50 randomly selected instances from each of the 12 instance classes. A time limit of 3600 seconds was used for the CMSA algorithm, while the exact cover procedure was set to run for a maximum of 600 seconds. Parameter settings of $p = 3$ and $maxAge = 3$ were decided after preliminary tests. \cmsa{Note that the best solution of the three recombination operators was used to compare EA with CMSA.}

\begin{table}[h!]
\centering
\caption{\cmsa{CMSA}}
\begin{threeparttable}
\begin{tabular}{lrrcrrrcrrr}
	\toprule
	& & & & \multicolumn{3}{c}{EA} &\phantom{ab}& \multicolumn{3}{c}{CMSA}\\
	\cmidrule{5-7} \cmidrule{9-11}
	\multicolumn{1}{c}{Type, $W$} & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$t$\tnote{$a$}} && \multicolumn{1}{c}{$|\mathcal{S}|$\tnote{$b$}} & \multicolumn{1}{c}{$\# t$\tnote{$c$}} & \multicolumn{1}{c}{$\mathcal{S}_{best}$\tnote{$d$}} && \multicolumn{1}{c}{$|\mathcal{S}|$} & \multicolumn{1}{c}{$\# t$} & \multicolumn{1}{c}{$\mathcal{S}_{best}$}\\
	\midrule
	a, $W=2500$ & 100 & 23.323 && 23.38 & 49 & 28 && 24.02 & 21 & 0 \\
	& 500 & 114.942 && 116.1 & 17 & 45 && 118.48 & 0 & 0 \\
	& 1000 & 229.437 && 234.84 & 0 & 38 && 237.1 & 0 & 4 \\
	\midrule
	a, $W=5000$ & 100 & 11.922 && 12.56 & 39 & 26 && 13.54 & 24 & 0 \\
	& 500 & 57.722 && 61.9 & 1 & 50 && 75.6 & 0 & 0 \\
	& 1000 & 114.965 && 126.4 & 0 & 50 && 169.04 & 0 & 0 \\
	\midrule
	\midrule
	r, $W=2500$ & 100 & 23.473 && 27.52 & 21 & 16 && 27.94 & 15 & 6 \\
	& 500 & 115.239 && 143.1 & 4 & 34 && 148.08 & 0 & 9 \\
	& 1000 & 229.946 && 290.32 & 0 & 22 && 294.76 & 0 & 23 \\
	\midrule
	r, $W=5000$ & 100 & 11.981 && 20.56 & 16 & 18 && 20.98 & 12 & 5 \\
	& 500 & 57.865 && 109.28 & 4 & 29 && 114.18 & 6 & 16 \\
	& 1000 & 115.227 && 224.52 & 0 & 31 && 239.38 & 0 & 15 \\
	\bottomrule
\end{tabular}
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 50 instances).
	\item[$b$] Number of bins per solution (mean from 50 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 50 instances).
	\item[$d$] Number of instances where the solution generated by the algorithm consists of fewer bins than the solution generated by the alternative algorithm.
\end{tablenotes}	
\end{threeparttable}	
\label{table:cmsa}
\end{table}

\begin{comment}
{\color{myGreen}
\begin{itemize}[leftmargin=*]
	\item Designed to not search for all covers, only minimum one, so recursion is restricted to as many levels as the best solution found so far.
	\item Ran EA on those 50 instances for an hour to compare fairly, and best solution from the three recombination operators is chosen.	
	\item $\mathcal{B}$ does not include duplicates.
	\item If no solution found within 600 seconds, set $\mathcal{S}^* \gets \mathcal{S}_{bsf}$, unless first iteration of CMSA and $\mathcal{S}_{bsf} \gets \emptyset$, then keep going until one solution found.
	\item When comparing in line 9, the ``better than'' is done by comparing the number of bins first and choosing solution with fewest bins, otherwise if number of bins equal then compare fitness values.
	\item Zenodo citation.
	\item From decision problem to optimisation problem.
	\item Compare by fitness value if number of bins equal in RS.
\end{itemize}
}
\end{comment}

%--------------------------------------------------------------------------------------


\section{Conclusion and Further Work}
\label{sec:conclusion}
{\color{myPurple}
\begin{itemize}[leftmargin=*]
	\item Lower bound.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\begin{comment}
\section{Checklist}
{\color{myAqua}
\begin{itemize}[leftmargin=*]
	\item Vicinal \emph{sum} constraint, not vicinal \emph{score} constraint.
	\item All dashes $'$ not '. 
	\item Abbreviations (SCPP, SubSCP, AHC etc.) spelling and being used correctly.
	\item Figure and table captions.
	\item Check that correct figures and equations are being referred to in text.
	\item Zenodo links DOI.
	\item State specification of computers used for experiments.
	\item Section and subsection titles, capitalisation and spelling.
	\item All figures have same line thickness, dashed line density and thickness, label size, vertex size, and colour (use tikz colours \texttt{tRed} and \texttt{tBlue}).
	\item All figures aligned correctly, subfigures aligned so that the captions are level.
	\item \texttt{$\backslash$noindent} only used when required, after equations, check if needed after definitions/figures/tables etc.
	\item Equations referenced using \texttt{$\backslash$eqref}, not \texttt{$\backslash$ref}.
	\item Tilde $\sim$ before all \texttt{$\backslash$ref} and \texttt{$\backslash$eqref}.
	\item Font/colours of figures clear, labels legible.
	\item American/British spelling.
	\item Word repetitions, duplicate statements.
	\item Footnotes, check if required or if statement can be put in text.
	\item Compare with previous paper.
	\item Table footnotes, font, spelling.
	\item Pseudocode?
	\item Dominating $\to$ universal.
	\item Use \texttt{$\backslash$dotsc} in all cases.
	\item Edges in parentheses $()$, not braces $\{\}$.
	\item Check correct notation used, $i, j, S_i, S_j$ etc.
	\item Do not use "OR" or Operational Research" etc.
	\item APA referencing, check references spelling and format, correct titles, years etc.
	\item MGPs referenced throughout paper, make sure it is actually stated in the introduction (might be removed from introduction because of new layout from Rhyd's suggestions - 04/04/2019)
	\item EPSRC?
	\item Check things that have been removed from intro are not directly referenced to or are placed elsewhere in the paper (e.g. BPP NP-hard)
	\item Strips $\to$ bins.
	\item SCSPP $\to$ SCPP.
	\item New citation style, check that sentences make sense, i.e. ``implemented in [1]...'' works but ``implemented in Becker (2010)...'' does not.
	\item Use correct citation command: citet, citep, citeauthor etc.
	\item Fulfil (one `l').
	\item ``red'' and ``blue'' edges, refer to them in figures, thick blue, thin red.
	\item Vertex weights.
	\item Check notation used in tikz figures is correct.
	\item Capital F for Figure in middle of sentence?
	\item Remove custom colours.
	\item Tense, ``the first operator we investigate'' or ``investigated''?
	\item Go through Elsevier Author Guide.
	\item Change all ``Figure'' to ``Fig.''.
	\item Align tables.
	\item Check all indentations after figures, tables etc.
	\item Compare Heuristics and EA results, number of types, six or twelve, classes or subtypes etc., be consistent.
	\item Partial solution or subsolution? Difference?
	\item All edges in curly braces, not parentheses.
	\item All $\mathcal{S}^* \gets \mathcal{S}'$; $\mathcal{S}^{**} \gets \mathcal{S}''$ 
	\item Largest/highest/smallest/lowest weight of vertex, choose.
\end{itemize}		
}
\end{comment}

\bibliographystyle{model5-names}
\bibliography{includes/bibliography}

\end{document}