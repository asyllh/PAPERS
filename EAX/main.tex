\documentclass[authoryear]{elsarticle}
\input{includes/preamble.tex}
\begin{document}
	
\begin{frontmatter}
\title{Evolutionary Methods for the Score-Constrained Packing Problem}
\author{Asyl L. Hawa}
\author{Rhyd Lewis}
\author{Jonathan M. Thompson}
\address{School of Mathematics, Cardiff University, Senghennydd Road, Cardiff, UK}
\begin{abstract}
\note{Type of packing problem in which order and orientation of items is important. Paper investigates heuristics, EA, postoptimisation.} 
\end{abstract}	
\end{frontmatter}

%--------------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
\noindent Many problems in operational research and discrete mathematics involve the grouping of elements into subsets. These types of problems can be seen in areas such as scheduling \citep{thompson1998, carter1996}, frequency assignment \citep{aardal2007}, graph colouring \citep{lewis2012, malaguti2008}, and load balancing \citep{rekiek1999} \intro{and compsci - table formatting, prepaging, file allocation \citep{garey1972}}. Formally, given a set $\mathcal{I}$ of $n$ elements, the aim is to produce a partition of subsets $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{i=1}^{k} S_i &= \mathcal{I}, & \label{eqn:allpacked}\\[3pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1, 2,\dotsc,k\}, \hspace{1mm} i \neq j, \text{ and}\label{eqn:nointersect}\\[3pt]
	S_i &\in \mathcal{F} & &\forall \hspace{1mm} i \in \{1,2,\dotsc,k\}.\label{eqn:feasible}
	\end{alignat}
\end{subequations}

\noindent Here, \eqref{eqn:allpacked} and \eqref{eqn:nointersect} state the requirement that every element must be in exactly one of the $k$ subsets. Equation~\eqref{eqn:feasible} then specifies that each subset $S_i \in \mathcal{S}$ must be feasible, where $\mathcal{F}$ is used to denote the set of all feasible subsets of elements in $\mathcal{I}$. The notion of feasibility is dependent on the specific constraint of the given problem. For example, in the graph colouring problem where vertices on a graph must be assigned colours such that no two adjacent vertices are in the same colour class, $\mathcal{F}$ contains all possible independent sets of vertices, whilst for the classical one-dimensional bin-packing problem (BPP), which requires a set of items of varying sizes to be packed into the fewest number of bins of fixed capacity, a bin $S_i$ is feasible if it is not overfilled.

The focus of this paper is on another packing problem which occurs in the packaging industry, where flat rectangular items of varying widths are to be cut and scored from fixed-length strips of cardboard to then be folded into boxes. This problem was originally introduced as an open-combinatorial problem by \citeauthor{goulimis2004} in 2004, and subsequently studied in \citet{lewis2011}, \citet{becker2015}, and \citet{hawa2018}.

Consider a set $\mathcal{I}$ of $n$ rectangular items of fixed height $H$. Each item $i \in \mathcal{I}$ has width $w_i \in \mathbb{Z}^+$ and is marked with two vertical score lines in predetermined places. The distance between each score line and the edge of the item are the score widths, $a_i, b_i \in \mathbb{Z}^+$ (where w.l.o.g. $a_i \leq b_i$) \intro{see Figure?}. A pair of knives mounted on a bar simultaneously cut along the score lines of two adjacent items, making it easier to fold the cardboard at a later stage. Due to the manner in which the knives are secured onto the bar, the knives must maintain a set distance from one another, a so-called ``minimum scoring distance'' $\tau \in \mathbb{Z}^+$, which is approximately 70mm in industry. In order for the knives to score all of the items in the correct locations, the distance between two score lines of adjacent items must exceed the minimum scoring distance. Hence, the following \emph{vicinal sum constraint} must be fulfilled:
\begin{equation}
	\textbf{r}(i) + \textbf{l}(i+1) \geq \tau \quad \forall \hspace{1mm} i \in \{1,2,\dotsc,|S|- 1\}
	\label{eqn:vsc}
\end{equation}

\noindent where \textbf{l}($i$) and \textbf{r}($i$) denote the left- and right-hand score widths of the $i$th item in bin $S$. Clearly if this constraint is met the distance between the score lines will be sufficient for the knives to be able to cut appropriately. Figure~\ref{fig:itemsknife} shows how adjacent items are scored simultaneously by pairs of knives. Although the vicinal sum constraint is met between items A and B, the full alignment of all three items is infeasible as the sum of the adjacent score widths of items B and C is less than the minimum scoring distance $\tau$, and so the knives are unable to move close enough together to score the lines in the required locations.
\begin{figure}[H]	
	\centering
	\includestandalone[width=0.7\textwidth]{figures/itemsknife}
	\caption{Dimensions of an item $i$ marked with dashed score lines, and an example packing showing both feasible and infeasible alignments of three items to be scored by pairs of knives. Here, the minimum scoring distance $\tau = 7$.}	
	\label{fig:itemsknife}
\end{figure}

\begin{definition}
	Let $\mathcal{I}$ be a set of $n$ rectangular items of height $H$ with varying widths $w_i$ and score widths $a_i, b_i$ $\forall$ $i \in \mathcal{I}$. Given a minimum scoring distance $\tau$, the Score-Constrained Packing Problem (SCPP) involves packing the items from left to right into the fewest number of $H \times W$ bins such that the vicinal sum constraint is satisifed in each bin and no bin is overfilled.
	\label{defn:scsp}
\end{definition}	

\noindent Each item $i \in \mathcal{I}$ can be packed into a bin in either a regular orientation, denoted $(a_i, b_i)$, where the smaller score width $a_i$ is on the left-hand side of item $i$, or a rotated orientation $(b_i, a_i)$, where the item has been rotated $180^{\circ}$ so that the larger score width $b_i$ is on the left-hand side. Thus, the problem involves finding an ordering and orientation of the items in the bins such that the sum of all adjacent score widths is greater than or equal to the minimum scoring distance $\tau$, i.e. the vicinal sum constraint is satisfied.\footnote{Note that the outermost score widths in each bin are disregarded as they are not adjacent to any other items.} For example, in Figure~\ref{fig:itemsknife}, observe that a feasible alignment of all three items can be obtained by rotating item C. As there are $2^{n-1} n!$ distinct orderings of $n$ items, it is clear that enumerative methods are not suitable. Figure~\ref{fig:bppvscpp} shows feasible solutions for the BPP and SCPP using the same set of items $\mathcal{I}$, where the SCPP requires an extra bin to accommodate all items without violating the vicinal sum constraint. Thus the BPP can be seen as a special case of the SCPP when $\tau = 0$, and so the vicinal sum constraint will always be satisfied.
\begin{figure}[H]
	\centering	
	\begin{subfigure}[h]{0.27\textwidth}
		\includestandalone[width=\textwidth]{figures/bpp}
		\caption{BPP}
		\label{fig:bpp}
	\end{subfigure} \hspace{20mm}
	\begin{subfigure}[h]{0.27\textwidth}
		\includestandalone[width=\textwidth]{figures/scppintro}
		\caption{SCPP}
		\label{fig:scpp}
	\end{subfigure}
	\caption{Solutions for the BPP and SCPP using the same set $\mathcal{I}$ of 10 items and $W = 10$. For the SCPP, $\tau = 7$.}	
	\label{fig:bppvscpp}
\end{figure}

\noindent One interesting problem related to the BPP is the Trapezoid Packing Problem (TPP), initially investigated by \citet{lewis2011}, where trapezoids are to be packed into bins so as to minimise the number of bins required, whilst also attempting to reduce the amount of triangular waste between adjacent trapezoids within each bin. Another problem similar to the BPP is the cutting stock problem (CSP). One particular case described by \citet{garraffa2016} considers sequence-dependent cut-losses (SDCL). Here, rectangular items of varying lengths are to be cut from strips of material of fixed lengths, however the type of cutting machine used results in material loss between items during the cutting process. The amount of loss can vary between different items, and is also dependent on the order of the items, i.e. a cut loss between two adjacent items A and B, with A packed first, may not necessarily be equal to the cut loss that arises when B is packed first. Hence, the CSP-SDCL involves packing the items into the fewest number of bins such that the sum of item lengths \emph{and} the sum of cut losses between all adjacent items in each bin does not exceed the bin capacity.

As with the TPP and CSP-SDCL, the SCPP not only involves deciding which bin each item should be packed into, but also, unlike the BPP, \emph{how} the items should be packed -- that is, determining the order and orientation of items within each bin. One specific difference, however, concerns the feasibility of individual bins. In the TPP, although clearly not optimal, it is still legal to place trapezoids with opposite angles, i.e. `$\backslash$' and `/', alongside one another. Likewise in the CSP-SDCL, two items with a large cut loss between them can still be packed alongside one another if necessary. Both of these problems allow items to be packed in \emph{any} order and orientation as long as the bins are not overfilled. In contrast, the SCPP possesses the strong vicinal sum constraint which if violated immediately causes an alignment of items in a bin to be invalid, thus rendering the entire solution infeasible. For consistency, we shall refer to the \intro{single-bin} problem of finding a legal sequence of a subset of items $\mathcal{I}' \subseteq \mathcal{I}$ in a \intro{single bin} as the Score-Constrained Packing Sub-Problem (SubSCP).

In the next section, we will provide a brief overview of the polynomial-time algorithm used to solve the SubSCP. Section~\ref{sec:scpp} will explain the difficulties associated with the SCPP and analyse bespoke heuristics \intro{created in a different paper}. An evolutionary algorithm for the SCPP is presented in Section~\ref{sec:ea}, along with results from rigorous experiments. Section~\ref{sec:postopt} details a postoptimisation procedure involving a recursive backtracking algorithm to improve upon results obtained from the EA, and finally Section~\ref{sec:conclusion} concludes the paper and discusses outcome and possible directions for further work.

% Notes - Introduction
{\color{myOrange}
\begin{itemize}[leftmargin=*]
	\idone{For the TPP and SDCL, order and orientation helps minimise waste material and find a better solution using fewer strips, but for the SCPP order and orientation is required to find a \emph{feasible} solution.}
	\idone{Two phsae approach using postoptimisation stage (like Malaguti \cite{malaguti2008}).}
	\item Items can be run through the machine individually but this incurs extra time/costs.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{The Alternating Hamltonian Construction Algorithm}
\label{sec:ahc}
\noindent Firstly, let us consider the following sequencing problem originally defined by \citet{hawa2018}:

\begin{definition} % COP
	\label{defn:cop}
	Let $\mathcal{M}$ be a multiset of unordered pairs of integers $\mathcal{M} = \{\{a_1, b_1\}, \{a_2, b_2\},\dotsc,\{a_n, b_n\}\}$, and let $\mathcal{T}$ be a sequence of the elements of $\mathcal{M}$ in which each pair is a tuple. Given a fixed value $\tau \in \mathbb{Z}^+$, the Constrained Ordering Problem (COP) consists of finding a solution $\mathcal{T}$ such that the sum of adjacent values from different tuples is greater than or equal to $\tau$.
\end{definition}

\noindent For example, given the COP instance $\mathcal{M} = \{\{1,2\}, \{1,6\}, \{2,3\}, \{2,4\}, \{3,5\}, \{4,5\}\}$ and a fixed value $\tau = 7$, one possible feasible sequence is $\mathcal{T} = \langle(1,6), (2,4), (3,5), (2,3), (4,5), (2,1)\rangle$. It is evident that the COP is in fact equivalent to the SubSCP, whereby each pair in $\mathcal{M}$ can be seen as an item $i$ represented by its score widths $a_i, b_i$, and the constraint value $\tau$ is the minimum scoring distance. It follows that the requirement for the sum of adjacent values to exceed $\tau$ corresponds to the vicinal sum constraint \eqref{eqn:vsc}.

In this section we present the Alternating Hamiltonian Construction (AHC) algorithm, a polynomial-time algorithm for solving the COP, and hence the SubSCP. The underlying algorithm was originally proposed by \citet{becker2010}, and determines whether a feasible solution exists for a given instance. This was then extended by \citet{hawa2018} so that if a solution does indeed exist AHC is able to construct the final solution. Here, we further simplify and increase the efficiency and speed of AHC by replacing two of the subprocedures used in the previous version of AHC with a single algorithm.

We begin by modelling a given instance $\mathcal{M}$ of the COP as a vertex-weighted graph $G$, which has a vertex set $V$ defined using one vertex for each element in $\mathcal{M}$ in non-decreasing order. Each vertex $v_i \in V$ is weighted with the value in $\mathcal{M}$ it represents. In order to prevent executing the algorithm unnecessarily, a basic preliminary test is performed to determine if the instance is infeasible. Of the $2n$ vertices, suppose the smallest vertices $v_1$ and $v_2$ are placed on the ends of the sequence. Clearly, if the next-smallest vertex $v_3$ and the largest vertex $v_{2n}$ do not meet the vicinal sum constraint, then there cannot exist a feasible ordering of all elements in $\mathcal{M}$. Note that a positive outcome from this test does not necessarily imply that a feasible solution exists for the instance, however a negative outcome confirms the non-existence of a solution.

If $\mathcal{M}$ has not been deemed infeasible an extra pair of vertices $v_{2n+1}, v_{2n+2}$ is added to $G$, each assigned a weight equal to $\tau$. $G$ now comprises two edge sets: $B$, which contains \ahc{blue} edges between vertices that are \emph{partners}, that is, whose weights make up an unordered pair in $\mathcal{M}$; and $R$, containing \ahc{red} edges between vertices that add up to $\geq \tau$ and are not partners. The additional vertices are partners, and can be seen to be universal vertices as they are adjacent to every other vertex via an edge in $R$; thus both have degree $2n+1$. Given the bijective function $p : V \to V$ that associates each vertex $v_i \in V$ with its partner $p(v_i)$, the set of edges between partners can be denoted as $B = \{(v_i, p(v_i)) : v_i \in V\}$. Note that $|B| = n+1$, and so $B$ is a perfect matching in all cases. Figure~\ref{fig:threshold} illustrates the resulting graph $G = (V, B \cup R)$ produced from the example instance $\mathcal{M}$ of the COP provided above. The graph has a noticeable pattern, with the \ahc{degree of each vertex increasing in accordance with the weight} of the vertices - \ahc{degree sequence}.

A Hamiltonian cycle in a graph $G$ is a cycle that visits every vertex of $G$ exactly once. A graph containing such a cycle is said to be Hamiltonian. From this, we define a specific type of Hamiltonian cycle:

\begin{definition} % Alternating Hamiltonian Cycle
	\label{defn:althamcycle}
	Let $G = (V, B \cup R)$ be a simple, undirected graph where each edge is a member of one of two sets, $B$ or $R$. $G$ contains an alternating Hamiltonian cycle if there exists a Hamiltonian cycle whose successive edges alternate between sets $B$ and $R$.
\end{definition}

\noindent It can be seen that an alternating Hamiltonian cycle in $G$ corresponds to a legal sequence of the elements in $\mathcal{M}$, as the edges in $B$ represent each pair of values in $\mathcal{M}$, and edges from $R$ depict the values that meet the vicinal sum constraint and can be ordered so that they are adjacent to one another. The solution $\mathcal{T}$ must contain all elements from the problem instance $\mathcal{M}$ so all $n+1$ edges in $B$ must be present in the alternating Hamiltonian cycle. Therefore, the aim is to find a suitable set of edges $R' \subseteq R$ that, together with the edges in $B$, form an alternating Hamiltonian cycle in $G$. The universal vertices are used to aid the construction of the alternating Hamiltonian cycle as they are able to connect to the smallest vertices, however once an alternating Hamiltonian cycle has been produced the universal vertices and any incident edges are removed, with the resulting path corresponding to a solution $\mathcal{T}$.

Determining whether a graph is Hamiltonian is one of the well-known 21 NP-complete problems \cite{karp1972}, whilst the problem of actually finding a Hamiltonian cycle is NP-hard. Consequently, the alternating Hamiltonian cycle problem is also NP-hard, as it is a generalisation of the Hamiltonian cycle problem \citep{haggkvist1977}. Despite this, due to the manner in which graphs are modelled from instances of the COP and the requirement that all edges in $B$ must be included, we are able to determine the existence of an alternating Hamiltonian cycle in a graph $G$ in polynomial time \citep{hawa2018}.

The first subprocedure of AHC is the Maximum Cardinality Matching (MCM) algorithm, which is used to produce a matching $R'$ from $R$ \citep{mahadev1994}. MCM takes each vertex $v_1, v_2,\dotsc,v_{2n+2}$ and adds to $R'$ the edge from $R$ connecting $v_i$ to the highest-indexed vertex $v_j$ that is not incident to an edge in already in $R'$. Pairs of vertices incident to edges in $R'$ are then said to be \emph{matched}. As with partners, the bijective function $m : V \to V$ associates with each vertex $v_i \in V$ its match, $m(v_i)$, thus the set can then be denoted as $R' = \{(v_i, m(v_i)): v_i \in V\}$. In the event that a vertex $v_i$ is not adjacent to any other vertex via an edge in $R$, the previous vertex $v_{i-1}$ can be rematched, provided 
\begin{enumerate*}[label={(\alph*)}]
	\item $i \neq 1$;
	\item $v_{i-1}$ has been matched; and
	\item $(v_{i-1}, p(v_i)) \in R$.
\end{enumerate*} 
Then, we simply set $m(v_i) = m(v_{i-1})$, and $m(v_{i-1}) = p(v_i)$.

If $R'$ does not contain $n+1$ edges then there are too few edges to form a cycle with the edges in $B$, thus no feasible solution can exist and AHC is terminated. Otherwise, the subgraph $G'=(V, B \cup R')$ is a 2-regular graph consisting of cyclic components $C_1,C_2,\dotsc,C_z$ (as demonstrated in Figure~\ref{fig:mcm}). Clearly, if $z = 1$, then $G'$ is an alternating Hamiltonian cycle and a solution has been found. However, if $z > 1$, then AHC must find a way of connecting these components together to form a single alternating Hamiltonian cycle.

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/threshold}
		\caption{$G = (V, B \cup R)$}
		\label{fig:threshold}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/matching}
		\caption{$G' = (V, B \cup R')$}
		\label{fig:matching}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.243\textwidth}
		\includestandalone[width=\textwidth]{figures/mps}
		\caption{$G' = (V, B \cup R')$ \ahc{planar}}
		\label{fig:mps}
	\end{subfigure}
	\caption{The graph $G$ created from the instance $\mathcal{M} = \{\{1,2\}, \{1,6\}, \{2,3\}, \{2,4\}, \{3,5\}, \{4,5\}\}$, and the spanning subgraph $G'$ formed using the set $R' \subseteq R$ produced by MCM. In planar form it can be seen that $G'$ comprises $z = 3$ cyclic components. \ahc{vertex weights in parentheses, universal vertices are $v_{13}$ and $v_{14}$.}}
	\label{fig:mcm}
\end{figure}

\noindent Recall that an edge in a graph is a \emph{bridge} if the removal of the edge increases the number of components of the graph, so the addition of a bridge decreases the number of components. In order to combine the multiple cyclic components of $G'$ into a single component, suitable edges must be selected from $R \backslash R'$ that are able to function as bridges between the different components. For this, AHC calls upon the Bridge-Cover Recognition (BCR) algorithm, which produces sets of edges $R_1, R_2, \dots$ that can be used to link the components of $G'$. A set $R_i$ is said to \emph{cover} a component $C_j$ if $R_i$ contains an edge from $C_j$. BCR operates by creating sets of edges that cover all $z$ components of $G'$.

In \citet{becker2010}, a procedure is used to find these sets of edges, which determine whether a feasible solution exists for a given instance. However, for some instances these sets are unable to connect the components of $G'$ into a single alternating Hamiltonian cycle, instead forming multiple cycles which do not correspond to a feasible solution. An additional procedure is implemented in \citet{hawa2018} which finds and produces a feasible solution to these instances, but we found that the original procedure was then unecessary. Here, we replace the two procedures with a single algorithm, BCR, that produces the same results in a more efficient manner.

Firstly, the edges in $R'$ are sorted into a list $\mathcal{L}$ such that the lower-indexed vertices of the edges are in increasing order and the higher-indexed vertices are in decreasing order \ahc{refer to list in figure~\ref{fig:bcr}}, and any edges that cannot be sorted in this manner are removed from the list. The BCR procedure then searches through $\mathcal{L}$ for an edge whose lower-indexed vertex is adjacent to the higher-indexed vertex of the next edge via an edge from $R \backslash R'$, and that is not in the same component of $G'$ as the next edge. This edge starts the set $R_1$, and succeeding edges are added to $R_1$ provided the adjacency condition holds and the succeeding edge is not a member of a component that is already covered by $R_1$. In the event that BCR reaches the end of the list and $R_1 = \emptyset$, then no suitable edges have been found that can join the components, thus there is no feasible solution to the given instance $\mathcal{M}$. On the other hand, if $|R_1| = z$ then $R_1$ covers all components of $G'$, and a connecting procedure is able to use $R_1$ to combine the components into an alternating Hamiltonian cycle.

Otherwise, $R_1$ is added to a collection $\mathcal{R}$ and the edges in $\mathcal{R}$ are removed from $\mathcal{L}$. An iterative procedure is then used to find overlapping sets. A set $R_i$ \emph{overlaps} with $\mathcal{R}$ if $R_i$ covers exactly one component that is covered by $\mathcal{R}$, and all other components covered by $R_i$ are not yet covered by $\mathcal{R}$. During each iteration, BCR searches from the beginning of $\mathcal{L}$ for edges that fulfil the adjacency condition and can form an overlapping set. If such a set is found, BCR adds it to $\mathcal{R}$ and continues through the list in search of another overlapping set. After the penultimate edge has been assessed, edges in $\mathcal{R}$ are removed from $\mathcal{L}$ and the next iteration begins. As soon as $\mathcal{R}$ contains overlapping sets that cover all components of $G'$, the process is immediately terminated. The connecting procedure is then used all on sets $R_i \in \mathcal{R}$ to combine the components of $G'$. If, however, no new sets are created during an iteration and $\mathcal{R}$ does not cover all components, then it is clear that no more sets exist and the components cannot be combined into a single alternating Hamiltonian cycle. Furthermore, if fewer than two edges remain in $\mathcal{L}$ after an iteration, then no more sets can be created as at least two edges are required to start a new set. In both of these cases, it can be said with absolute certainty that no feasible solution exists for the given instance $\mathcal{M}$ of the COP.

The connecting procedure operates on a set $R_i$ as follows: for each edge in $R_i$ in turn, the edge from $R \backslash R'$ connecting the lower-indexed vertex of the edge to the higher-indexed vertex of the next edge is added to $R'$ (for the last edge, take the first edge in $R_i$ to be the next edge). The edges in $R_i$ that appear in $R'$ are then removed, resulting in $n+1$ edges in $R'$ that form an alternating Hamiltonian cycle on $G'$ with the edges in $B$. An example of this is shown in Figure~\ref{fig:bcr}, where BCR has produced a single set $R_1 = \{(v_3, v_{12}),(v_4, v_{11}), (v_5, v_{10})\}$. 

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bcrandcp}
		\label{fig:bcrcp}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpsconnect}
		\label{fig:mpsconnect}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpscycle}
		\label{fig:mpscycle}
	\end{subfigure}
	\begin{subfigure}[h]{0.55\textwidth}
		\includestandalone[width=\textwidth]{figures/solutionpath}
		\label{fig:solutionpath}
	\end{subfigure}
	\caption{Bridge-Cover Recognition (BCR) algorithm and connecting procedure used to combine the components of $G'$ to produce a single alternating Hamiltonian cycle that corresponds to a solution $\mathcal{T}$. \ahc{Explain final path removed universal vertices.}}
	\label{fig:bcr}
\end{figure}

\begin{theorem}
	\label{thm:ahc}
	Let $G=(V, B \cup R)$ be a graph created from an instance $\mathcal{M}$ of cardinality $n$ of the COP. Then, AHC terminates in at most $O(n^2)$ time.
\end{theorem}

\begin{proof}
	Firstly, MCM produces a matching set $R' \subseteq R$ in at most $O(n \lg n)$ time due to the sorting of the vertices in lexicographical order. In BCR, sorting the $n+1$ edges of $R'$ and removing unsuitable edges also requires $O(n \lg n)$ time. As each set $R_i$ contains at least two edges, BCR can produce up to $\frac{n+1}{2}$ sets. Since $G'$ comprises a maximum of $\frac{n+1}{2}$ components, it follows that the number of edge sets in $\mathcal{R}$ needed to connect all the components is bounded by $\frac{n+1}{2}-1$. The initial sorted list consists of at most $n+1$ edges, and therefore BCR is of quadratic complexity $O(n^2)$. Finally, the connecting procedure replaces up to $n+1$ edges, and so can be executed in $O(n)$ time. Consequently, AHC has an overall worst case complexity of $O(n^2)$ \ahc{rewrite}.
\end{proof}	

%--------------------------------------------------------------------------------------
\section{Heuristics for the SCPP}
\label{sec:scpp}
\noindent A feasible solution for an instance of the SCPP is represented by the set $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{j=1}^{k} S_j &= \mathcal{I}, & \label{eqn:packall}\\[3pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1,2,\dotsc,k\}, \hspace{1mm} i \neq j, \label{eqn:nooverlap} \\[3pt]
	A(S_j) = \sum\nolimits_{i=1}^{|S_j|}w_i &\leq W &\quad &\forall \hspace{1mm} S_j \in \mathcal{S}, \label{eqn:capacity} \\[3pt]
	\textup{\textbf{r}}(i) + \textup{\textbf{l}}(i+1) &\geq \tau &\quad &\forall \hspace{1mm} i \in \{1, 2,\dotsc,|S_j|-1\}, \hspace{1mm} \forall \hspace{1mm} S_j \in \mathcal{S}. \label{eqn:vscbin}
	\end{alignat}
\end{subequations}

\noindent An optimal solution for the SCPP is a solution consisting of the fewest number of bins required to feasibly pack all items in $\mathcal{I}$, thus the aim is to minimise $k$. Here, a bin $S_j \in \mathcal{F}$ if the total width of items in the bin does not exceed the bin's capacity \eqref{eqn:capacity} and the vicinal sum constraint is fulfilled \eqref{eqn:vscbin}. It follows that the SubSCP arises within the SCPP.

Since the SCPP generalises the BPP, it follows that the SCPP is also NP-hard. Assuming $P \neq NP$, we cannot hope to find an optimal solution for all instances of the SCPP in polynomial time. The simplest methods to implement for such problems are heuristics, which trade optimality for speed. One of the most well-known heuristics for the BPP is First-Fit (FF), a greedy online algorithm that packs each item, given in some arbitrary order, into the lowest-indexed bin such that the capacity of the bin is not exceed, opening a new bin when required. An improvement on FF yields the First-Fit Decreasing (FFD) heuristic, which initially sorts the items in non-increasing order of size. Similar heuristics include Best-Fit (BF), in which each item is packed into the fullest bin that can accommodate the item without being overfilled, and its offline counterpart Best-Fit Decreasing (BFD). A comprehensive overview of these heuristics and related methods can be seen in \citet{coffman1984}. More advanced heuristics for the BPP have been developed with positive results, such as the Minimum Bin Slack (MBS) heuristic \citep{gupta1999}, which focuses on packing each bin in turn rather than each item, and modifications of MBS such as the Perturbation-MBS' heuristic of \citet{fleszar2002}. 

For the BPP, a basic lower bound for $k$ is the theoretical minimum, $t = \ceil{\sum_{i=1}^{n} w_i / W}$ \citep{martello1990l}, however it can be seen that $t$ will not perform as accurately for the SCPP. Given a set of $n$ items in which the largest score width $b_i < \tau / 2$, it is clear that no pairs of score widths will fulfil the vicinal sum constraint and so each item must be packed into individual bins, thus $|\mathcal{S}| = n$. The theoretical minimum $t$ does not consider the effect of the minimum scoring distance on the feasibility of the solution.

The vicinal sum constraint also introduces further differences in solutions for the BPP and SCPP. The obvious disparity is that of the ordering and orientation of the items in the bins: unimportant in the BPP, but vital for the feasibility of a solution for the SCPP. Another distinction arises when attempting to modify solutions. A solution for the BPP remains feasible when an item is removed or a new item is added to a bin (provided the bin can accommodate said item), whereas this may render a solution for the SCPP infeasible, as the new adjacent score widths may not abide by the vicinal sum constraint. Because of these differences, basic heuristics designed for the BPP cannot be used for the SCPP as there is no guarantee that the final solution will be feasible. Notice however that these heuristics can still produce feasible, albeit not optimal, solutions for the TPP and CSP-SDCL.

% An example of this is depicted in Figure~\ref{fig:ffd} where FFD has been used for an instance of the SCPP, producing invalid alignments of items in three of the bins thus resulting in an entirely infeasible solution.

\begin{comment}
\begin{figure}[H]	
	\centering
	\includestandalone[width=0.4\textwidth]{figures/ffd}
	\caption{An infeasible solution produced using FFD for an example instance of the SCPP. Here, $|\mathcal{I}| = 15$, $W = 1000$, and $\tau = 70$. \scpp{$A(\mathcal{I}) = 5268$, therefore $t = 6$, shaded is unused material, item widths in middle of items.}}	
	\label{fig:ffd}
\end{figure}
\end{comment}

As the SCPP is a relatively new problem, few methods have been seen in literature. Some basic heuristics were introduced in \citet{hawa2018}, two of which are based on the FFD heuristic for the BPP. The first, named the Modified First-Fit Decreasing (MFFD) heuristic, performs in the same fashion as FFD with the extra condition that an item $i$ can only be packed into a bin $S_j$ if the score width on the end of $S_j$ and one of the score widths $a_i$ or $b_i$ meet the vicinal sum constraint. An improvement on MFFD is their second heuristic, MFFD$^+$, which incorporates AHC. Rather than attempting to pack each item into the ends of bins, MFFD$^+$ calls upon AHC to find a feasible ordering of all items currently in a given bin \emph{and} the item $i$ to be packed, i.e. AHC is used for every instance of SubSCP.\footnote{If an item $i$ is the first to be packed into a bin, it is placed in a regular orientation, $(a_i, b_i)$.} Clearly MFFD$^+$ is the superior of the two, as the application of AHC guarantees that a feasible ordering will be found if it exists. The restriction on MFFD of only packing items in the ends of bins has the potential to increase the number of bins being used in a final solution unnecessarily. \scpp{Figure~\ref{fig:mffdvsmffdplus} compares solutions produced using MFFD and MFFD$^+$ using the same instance of the SCPP as seen in Figure~\ref{fig:ffd}. Note how MFFD$^+$ is able to form a solution using the same number of bins as FFD while still satisfying the vicinal sum constraint.}

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.4\textwidth}
		\includestandalone[width=\textwidth]{figures/mffd}
		\caption{MFFD}
		\label{fig:mffd}
	\end{subfigure} \hspace{10mm}
	\begin{subfigure}[h]{0.4\textwidth}
		\includestandalone[width=\textwidth]{figures/mffdplus}
		\caption{MFFD$^+$}
		\label{fig:mffdplus}
	\end{subfigure}
	\caption{\scpp{Solutions produced using MFFD and MFFD$^+$ using the same instance as in the previous figure. MFFD$^+$ using same number of bins as FFD but feasible, see that MFFD$^+$ items can be rearranged in bins but not in MFFD.}}
	\label{fig:mffdvsmffdplus}
\end{figure}

% Notes - SCPP
{\color{myPink}
\begin{itemize}[leftmargin=*]
	\item Run MFFD and MFFD$^+$ experiments again, output time, and run them again using AHC from previous paper (without preliminary test and using BR/MBR, not BCR) to compare the time, also check for errors in strips/solution altHam vector.
	\item It is known that there always exists at least one ordering of the items such that FF produces an optimal solution \citet{lewis2009}.
	\item In 2007, it was proven that the worst case for FFD is $\frac{11}{9}k + \frac{6}{9}$, and that this bound is tight \citet{dosa2007}. Due to the initial sorting of the items, the time complexity of FFD is $O(n \lg n)$.
\end{itemize}
}

\subsection{Experimental Results - Heuristics}
\label{sub:expheuristics}
\noindent Although a comparison of these heuristics is given in \citet{hawa2018}, we performed out own tests where AHC used in MFFD$^+$ includes the preliminary test and the new BCR procedure. For our experiments, we produced two different types of instances: ``artificial'', in which the items are strongly heterogeneous (the items have varying widths and score widths); and ``real'', where items are weakly hetergeneous (many items have the same dimensions). For each type 1000 instances were generated using sets of 100, 500, and 1000 items, giving a total of 6000 problem instances. The minimum scoring distance $\tau$ was set to 70mm for all instances - the industry standard. All items have widths $w_i \in [150,1000]$ and score widths $a_i, b_i \in [1,70]$ selected uniform randomly, and equal height $H=1$. Two different bin sizes, $W = 2500$ and 5000 were used to alter the number of items per bin. As optimal solutions are not available, we calculated the solution quality $q = |\mathcal{S}|/ t$ which compares each solution to the theoretical minimum. Table~\ref{table:heuristics} contains the results from the experiments. All instances were completed in under 125ms.

\begin{table}[h!]
\centering
\caption{MFFD vs MFFD$^+$}
\begin{threeparttable}
\begin{tabular}{c@{\hspace{20pt}}c@{\hspace{20pt}}c@{\hspace{20pt}}c@{\hspace{20pt}}c@{\hspace{20pt}}ccc@{\hspace{20pt}}c@{\hspace{20pt}}c@{\hspace{8pt}}}\toprule
	& & & \multicolumn{3}{c}{MFFD} &\phantom{a}& \multicolumn{3}{c}{MFFD$^+$}\\
	\cmidrule{4-6} \cmidrule{8-10}
	Type, $n$ & $W$ & $t$\tnote{$a$} & $|\mathcal{S}|$\tnote{$b$} & $\# t$\tnote{$c$} & $q$\tnote{$d$} && $|\mathcal{S}|$ & $\# t$ & $q$\\ \midrule	
	a, 100 & 2500 & 23.323 & 30.754 & 0 & 1.320 && 28.457 & 26 & 1.221 \\
	a, 100 & 5000 & 11.922 & 23.583 & 0 & 1.982 && 19.881 & 7 & 1.670  \\
	\midrule
	a, 500 & 2500 & 114.942 & 140.206 & 0 & 1.220 && 132.647 & 0 & 1.154 \\
	a, 500 & 5000 & 57.722 & 103.209 & 0 & 1.789 && 89.544 & 0 & 1.552 \\
	\midrule
	a, 1000 & 2500 & 229.437 & 271.919 & 0 & 1.185 && 258.388 & 0 & 1.126 \\
	a, 1000 & 5000 & 114.965 & 198.325 & 0 & 1.725 && 172.613 & 0 & 1.502 \\
	\midrule
	\midrule
	r, 100 & 2500 & 23.473 & 37.069 & 5 & 1.600 && 35.419 & 16 & 1.523 \\
	r, 100 & 5000 & 11.981 & 32.348 & 1 & 2.731 && 29.611 & 5 & 2.497 \\
	\midrule
	r, 500 & 2500 & 115.239 & 184.106 & 0 & 1.612 && 177.249 & 0 & 1.551 \\
	r, 500 & 5000 & 57.865 & 163.819 & 0 & 2.860 && 153.416 & 0 & 2.678 \\
	\midrule
	r, 1000 & 2500 & 229.946 & 368.453 & 0 & 1.617 && 355.042 & 0 & 1.557 \\
	r, 1000 & 5000 & 115.227 & 328.618 & 0 & 2.882 && 308.642 & 0 & 2.706 \\
	\bottomrule
\end{tabular}
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
	\item[$d$] $q = |\mathcal{S}| /t$ (mean from 1000 instances).
\end{tablenotes}	
\end{threeparttable}	
\label{table:heuristics}
\end{table}

It can be seen that MFFD$^+$ produces solutions using fewer bins on average than MFFD for all 12 instance classes. MFFD was only able to find solutions using $t$ bins for two instance classes, whereas MFFD$^+$ produced solutions using $t$ bins for four instances classes. Solutions using fewer bins were found less frequently on average when the bin size is larger. The solution quality $q$ for the six instance classes where $W = 5000$ is significantly higher than when $W = 2500$. A larger bin size means the bins can accommodate more items, however this increases the number of adjacent score widths that must fulfil the vicinal sum constraint. Note also that $q$ is higher for real instances in comparison to artificial instances, as due to the reduced variety of item widths and score widths it is difficult to attain feasible packings. 

Indeed, it may be that optimal solutions were found in many of these instances, as the lower bound for $k$ may actually be higher than the theoretical minimum calculated. These experiments provide clear results that the use of an exact polynomial-time algorithm, AHC, within a heuristic is more powerful than using a simple heuristic alone. In spite of this, MFFD$^+$ lacks the capability of rearranging items \emph{between} bins, that is, once an item has been packed into a bin, it must stay in that bin (although the position of the item in the bin can change due to the use of AHC). It follows that every item packed has an effect on where the unplaced items can be placed, i.e. where an item is packed is dependent on where previous items have been packed. These limitations associated with greedy heuristics lead us to explore other superior methods. \scpp{where/how each item is packed is dependent on where/how previous items have been packed.}

% Notes - Experiments Heuristics
{\color{myPink}
\begin{itemize}[leftmargin=*]
	\item Number of types on average real instances.
	\item Number of items per bin.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{An Evolutionary Algorithm for the SCSPP}
\label{sec:ea}
\noindent We now introduce an evolutionary algorithm (EA) for the SCPP \ea{to improve on the results of the previous heuristics}. An evolutionary algorithm is a metaheuristic optimisation algorithm inspired by the natural evolutionary process. Candidate solutions to the problem form the initial population, and procedures emulating selection, reproduction, recombination and mutation are used to create the next generation of solutions. This iterated process results in the evolution of the solutions. Each solution is evaluated based on a given critera, and individuals which are more suited to the environment are given more oppportunity to breed while those which are less so are eliminated. EAs have been used for a variety of grouping problems with positive results \citep{lewis2017, falkenauer1996, quiroz2015}. \ea{Iterative procedure, plus local search and xover, swapping items between bins, better than previous heuristics as repetition means more possible item combinations in bins, EA has the ability to move items between bins.}

Within the EA framework, we investigate three different recombination operators as well as a local search procedure inspired by \citet{martello1990l}. AHC is also integrated into the EA to solve instances of the SubSCP.

% Notes - EA Introduction
{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item EA consists of multiple iterations, previous heuristics only one.
\end{itemize}
}

\subsection{Recombination}
\label{sub:xover}
\noindent A recombination operator is used to generate new solutions from an existing population by taking two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, and combining them to create a new offspring solution. The operator determines which characteristics from each parent should be inherited by the offspring, with the aim of retaining the best elements of each parent such that a superior offspring is produced. As described in the previous section, individual items cannot be copied from parent to offspring as this may cause a violation of the vicinal sum constraint. Therefore, the operators used in this EA have been designed to ensure offspring feasibility.

\begin{figure}[H]	
	\centering
	\includestandalone[width=0.75\textwidth]{figures/eavsc}
	\caption{removed individual items, causes vsc violation.}	
	\label{fig:eavsc}
\end{figure}

The first operator is based on the grouping genetic algorithm (GGA) of \citet{falkenauer1992}. The bins of the second parent solution $\mathcal{S}_2$ are permuted and two bins $S_i$ and $S_j$ from $\mathcal{S}_2$ are selected randomly (where $1 \leq i < j \leq |\mathcal{S}_2|$). All bins between and including $S_i$ and $S_j$ are inserted into an offspring solution $\mathcal{S}$. GGA then adds to the offspring all bins from $\mathcal{S}_1$ that do not contain items already present in the offspring. Note that the bins chosen from $\mathcal{S}_2$ cannot both be the outermost bins, that is, GGA cannot choose both $i = 1$ \emph{and} $j = |\mathcal{S}_2|$, as doing so would result in all bins from $\mathcal{S}_2$ being copied into the offspring, preventing the addition of any bins from $\mathcal{S}_1$. \ea{What is GGA's aim?}

Another operator we implemented is the alternating grouping crossover (AGX), and is analogous to that of \citet{quiroz2015}. Starting with the parent solution containing the fullest bin, AGX inserts this bin into an offspring solution $\mathcal{S}$, and bins containing items in $\mathcal{S}$ are removed from the other parent.\footnote{For both AGX and AGX', in the event that both parents contain bins with equal maximum fullness/number of items, the initial parent solution is chosen at random.} The operator then inserts the fullest bin from the modified parent into $\mathcal{S}$ and removes bins from the first parent. AGX continues to alternate between parents, selecting the fullest bin $max_{S_j \in \mathcal{S}_p} (A(S_j))$ (where $\mathcal{S}_p$ is the parent solution under consideration, $p \in \{1,2\}$) until $min (|\mathcal{S}_1|,|\mathcal{S}_2|) - 1$ bins have been added to the offspring solution. \ea{How is AGX useful? Choose fullest bin, less waste, less bins required.}

Our final operator, AGX$'$, performs in a similar manner to AGX, however rather than choosing the fullest bin to insert into the offspring solution AGX$'$ selects bins containing the most items, $max_{S_j \in \mathcal{S}_p} (|S_j|)$. This method has the ability to preserve bins containing items that are harder to pack along with other items. 

In order to maintain feasibility, the operators remove entire bins containing duplicate items rather than individual items. These bins may also contain items that are not present in the offspring solution. Consequently, on completion of the crossover, the offspring solution $\mathcal{S}$ may not contain all $n$ items and is therefore not yet a full solution. To rectify this, MFFD$^+$ is applied using the missing items to form a partial solution $\mathcal{S}^*$. The partial offspring solution $\mathcal{S}$ and $\mathcal{S}^*$ are then used as input into a local search procedure to create a full feasible offspring solution.

\subsection{Local Search}
\label{sub:localsearch}
\noindent Our local search method takes in two feasible partial solutions, $\mathcal{S}$ and $\mathcal{S}^*$, permutes the bins of both, and then attempts to move items between the two solutions in four stages: 
\begin{enumerate*}[label={(\roman*)}]
	\item swapping a pair of items from a bin in $\mathcal{S}$ with a pair of items from a bin in $\mathcal{S}^*$;\label{item:pairpair}
	\item swapping a pair of items from a bin in $\mathcal{S}$ with an individual item from a bin in $\mathcal{S}^*$;\label{item:pairsin}
	\item swapping individual items from bins in $\mathcal{S}$ and $\mathcal{S}^*$;\label{item:sinsin} and
	\item moving an item from a bin in $\mathcal{S}^*$ to a bin in $\mathcal{S}$.\label{item:movesin}
\end{enumerate*} 
During stages \ref{item:pairpair}--\ref{item:sinsin}, the width of the item(s) from $\mathcal{S}^*$ must exceed the width of the item(s) from $\mathcal{S}$. Once a swap or move has been perfomed, the procedure immediately moves on to the next stage. This method is repeated until all four stages have been executed in succession with no changes to $\mathcal{S}$ or $\mathcal{S}^*$. Then, MFFD$^+$ is applied to any items remaining in $\mathcal{S}^*$, generating a new feasible partial solution $\mathcal{S}^{**}$, with $|\mathcal{S}^{**}| \leq |\mathcal{S}^*|$. The bins in $\mathcal{S}^{**}$ are then inserted into $\mathcal{S}$ to form a full feasible solution.

This method is based on the dominance criterion of \citet{martello1990l}: if a bin $S_x$ \emph{dominates} a bin $S_y$, then a solution containing $S_x$ will have no more bins than a solution containing $S_y$. The local search procedure is in fact a local search for dominating bins. By moving larger items into $\mathcal{S}$, the fullness $A(S_j)$ of the bins $S_j \in \mathcal{S}$ increases whilst the number of items per bin is maintained or decreases, improving the \ea{quality} of the bins in $\mathcal{S}$. Simultaneously, items moved into $\mathcal{S}^*$ are smaller and therefore easier to repack into bins in $\mathcal{S}$ during stage~\ref{item:movesin}. Variations of this method can be seen in \citet{lewis2009, lewis2017, falkenauer1996, levine2004}, however the addition of the vicinal sum constraint results in fewer changes than seen in these previous implementations. By iterating the stages numerous distinct subsets of items in the bins are produced, generating more possibilities for feasible orderings of items.

% Notes - Local Search
{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item ``Note that this entire procedure cannot increase the number of bins being used in a solution, but it does have the potential to decrease it.''
\end{itemize}
}

\subsection{Evolutionary Algorithm Framework}
\label{sub:eaframework}
\noindent Our EA for the SCPP begins by producing candidate solutions to form an initial population, with one solution created using MFFD$^+$ and the rest using MFFR$^+$ (the same method as MFFD$^+$ with the items in a random order). Each solution is mutated before being inserted into the population. The mutation of a solution $\mathcal{S}$ involves permuting the bins and inserting $1 < r < |\mathcal{S}|$ bins selected randomly from $\mathcal{S}$ into a set $\mathcal{S}^*$. Local search is then executed using these two partial solutions to produce a full feasible solution $\mathcal{S}$. 

An iteration of the EA \ea{involves} selecting two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, from the population at random, applying a recombination operator to produce an offspring solution $\mathcal{S}$, then finally mutating $\mathcal{S}$ before replacing the least fit of the two parents in the population.

A tailored function is used to determine the fitness of a solution, as opposed to simply relying on the number of bins within a solution. The reason for this is two-fold: firstly, given two solutions of equal size, it is impossible to determine the fitter solution based on the number of bins alone. Secondly, we note that the fitness of a solution not only depends on the number of bins used, but also \emph{how} the items are packed into the bins. It is clear that if the bins' capacities are taken advantage of, fewer bins will be needed to pack all items compared to if the bins are only half full, requiring more bins than necessary to accommodate all of the items. A solution comprising fuller bins may also contain bins that are nearly empty, which is beneficial as it allows futher items to be packed, or the residual material could be used for other means. 

We therefore make use of the following function to calculate the fitness of a solution $\mathcal{S}$ \citep{falkenauer1992}:
\begin{equation}
	f(\mathcal{S}) = \frac{\sum_{S_j \in \mathcal{S}} (A(S_j)/W)^2}{|\mathcal{S}|}
\end{equation}

\noindent which assigns higher values to fitter solutions. This function exerts evolutionary pressure by favouring solutions with superior parkings, reducing the ability for less fit solutions to reproduce. Note also that if $|\mathcal{S}_1| < |\mathcal{S}_2|$  then $f(\mathcal{S}_1) > f(\mathcal{S}_2)$, hence a global optimum for this fitness function is associated with the optimal solution containing the fewest number of bins.

It is also important to consider how the vicinal sum constraint affects the fitness of a solution. Given two half-full bins containing items from an instance of the classical BPP, it may be possible to move items between the bins such that one bin is almost full and the other nearly empty. Now, suppose the items in the half-filled bins have score lines, and are from an instance of the SCPP. Obviously, it is more difficult to shuffle the items between the bins as the vicinal sum constraint may be violated. Thus, fuller bins in the SCPP are extremely valuable, and the fitness function ensures that solutions containing fuller bins are preserved in the population throughout generations.


\subsection{Experimental Results - EA}
\label{sub:expea}
\noindent To allow for a fair comparison with the heuristics in Section~\ref{sec:scpp}, the same six sets of problem instances from the previous experiments were used to obtain results from our EA. Each of the instance classes were executed six times using different recombination operators and bin sizes, producing 36 subclasses overall. We settled for an initial population containing 25 candidate solutions, which was shown to produce the best results. The advantage of using a small population size was to be expected, as the vicinal sum constaint limits the number of feasible candidiate solutions using a reasonable number of bins. It was seen that larger population sizes contained an abundance of poor solutions comprising over twice the number of bins as used in the final solution, whilst even smaller populations lacked the diversity needed to escape local optima. Across all instances, the EA had a fixed time limit of 300 seconds. Table~\ref{table:ea} displays the results obtained from the experiments.

\begin{table}[h!]
\centering
\caption{\ea{EA comparisons}}
\begin{threeparttable}
\begin{tabular}{c@{\hspace{10pt}}c@{\hspace{10pt}}c@{\hspace{15pt}}c@{\hspace{10pt}}c@{\hspace{10pt}}ccc@{\hspace{10pt}}c@{\hspace{10pt}}ccc@{\hspace{10pt}}c@{\hspace{10pt}}c}\toprule
	& & & \multicolumn{3}{c}{GGA} &\phantom{}& \multicolumn{3}{c}{AGX} &\phantom{}& \multicolumn{3}{c}{AGX$'$}\\
	\cmidrule{4-6} \cmidrule{8-10} \cmidrule{12-14}
	Type, $n$& $W$ & $t$\tnote{$a$} & $|\mathcal{S}|$\tnote{$b$} & $\# t$\tnote{$c$} & $q$\tnote{$d$} && $|\mathcal{S}|$ & $\# t$ & $q$ && $|\mathcal{S}|$ & $\# t$ & $q$\\ \midrule \midrule
	a, 100 & 2500 & 23.323 & 23.368 & 966 & 1.002 && 23.349 & 981 & 1.001 && 23.36 & 971 & 1.002\\
	a, 100 & 5000 & 11.922 & 12.304 & 833 & 1.033 && 12.352 & 810 & 1.037 && 12.353 & 807 & 1.037\\
	\midrule
	a, 500 & 2500 & 114.942 & 116.694 & 268 & 1.015 && 116.966 & 224 & 1.018 && 116.56 & 291 & 1.014\\
	a, 500 & 5000 & 57.722 & 62.393 & 43 & 1.081 && 62.813 & 35 & 1.088 && 63.022 & 37 & 1.092\\
	\midrule
	a, 1000 & 2500 & 229.437 & 234.938 & 7 & 1.024 && 235.458 & 10 & 1.026 && 234.409 & 13 & 1.022 \\
	a, 1000 & 5000 & 114.965 & 127.673 & 0 & 1.111 && 128.037 & 0 & 1.114 && 128.122 & 0 & 1.114 \\
	\midrule \midrule
	r, 100 & 2500 & 23.473 & 26.108 & 551 & 1.116 && 26.146 & 558 & 1.117 && 26.029 & 554 & 1.112 \\
	r, 100 & 5000 & 11.981 & 17.667 & 470 & 1.485 && 17.718 & 458 & 1.489 && 17.692 & 449 & 1.487\\
	\midrule
	r, 500 & 2500 & 115.239 & 134.642 & 33 & 1.172 && 134.74 & 47 & 1.173 && 134.49 & 36 & 1.171 \\
	r, 500 & 5000 & 57.865 & 94.491 & 81 & 1.643 && 95.009 & 78 & 1.652 && 94.76 & 74 & 1.648 \\
	\midrule
	r, 1000 & 2500 & 229.946 & 270.714 & 0 & 1.182 && 270.862 & 2 & 1.182 && 270.782 & 0 & 1.182 \\
	r, 1000 & 5000 & 115.227 & ER1051 & - & - && ER1052 & - & - && ER1053 & - & - \\
	\bottomrule
\end{tabular}	
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
	\item[$d$] $q = |\mathcal{S}| /t$ (mean from 1000 instances).
\end{tablenotes}
\end{threeparttable}
\label{table:ea}
\end{table}

{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item Time graph output.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{Postoptimisation}
\label{sec:postopt}
\noindent Given a collection $\mathcal{S}$ of subsets of a set $X$, an \emph{exact cover} is a subcollection $\mathcal{S}^*$ of $\mathcal{S}$ such that each element in $X$ is contained in exactly one subset in $\mathcal{S}^*$.

Consider an $m\times n$ binary matrix $X$. Let $M = \{1,2,\dotsc,m\}$ and $N = \{1,2,\dotsc,n\}$ be the rows and columns of the matrix respectively. Now, suppose each row $i \in M$ represents a bin, and each column $j \in N$ represents an item. Then, an element of the matrix $x_{ij} = 1$ iff item $j$ is in bin $i$. It can be said in this case that row $i$ \emph{covers} column $j$. Thus, the problem is to find a minimum cardinality subset of rows $S \subseteq M$ such that each column $j \in N$ is covered by exactly one row $i \in S$. This problem can be formulated as the following integer linear program.

\begin{subequations}
	\begin{alignat}{3}
		\text{minimise  } &\sum_{i \in M} c_i & \\[3pt]
		\text{subject to  } &\sum_{i \in M} x_{ij} c_i = 1 &\quad &\forall \hspace{1mm} j \in N \\[3pt]
		&c_i \in \{0,1\} & &\forall \hspace{1mm} i \in M
	\end{alignat}
\end{subequations}

\[c_i =
\begin{cases} 
1 & \text{if } i \in S \\
0 & \text{otherwise} 
\end{cases}
\]

\noindent The exact cover problem, determining whether a subcollection $\mathcal{S}^*$ exists, is a decision problem, and one of Karp's 21 NP-complete problems \cite{karp1972}. However, if we know that an exact cover does indeed exist, we can alter the problem to instead find the smallest subcollection.

Given a set of feasible bins, we can use this to find a solution to the SCPP. Since the bins provided are already feasible, the complications associated with the vicinal sum constraint are eliminated.

One method of solving the exact cover problem is by using a recursive depth-first backtracking algorithm, dubbed ``Algorithm X'' by Donald Knuth \cite{knuth2000}, which is used to find all solutions. Given our problem, it is unecessary to find all solutions. Instead, we have adapted the procedure to only seach for solutions that improve upon the best solution found thus far. Ideally, we would use the entire set of feasible bins $\mathcal{F}$ to form the matrix, however this could include hundreds of millions of feasible bins. Instead, we chose to use the set $\mathcal{A}$ created during the EA, which contains feasible bins found during the algorithm.

{\color{myGreen}
\begin{itemize}[leftmargin=*]
	\item Cite \citet{malaguti2008}.
	\item Exact cover formulation, NP-hard, state the IP, describe DLX.
	\item Cite \citet{knuth2000} dancing steps.
	\item Use strips in feasPacking for post opt.
	\item Compare with EA output - is a better solution found, or a solution with the same number of strips but a better fitness value?
	\item Is the post opt phase able to find a solution equal to the lowerbound?
	\item Post opt will only ever find a solution equal to or better than the solution found in EA, never worse.
	\item Execution time.
	\item Set cover problem is optimisation problem, find min number of sets.
	\item Exact cover problem is decision problem, does a set exist.
	\item However since we add every item on its own strip to feasPacking, we know that a set exists.
	\item Problem is to find minimum number of strips that covers all items and contains every item exactly once.
	\idone{$X = (x_{ij})$ - $m$ x $n$ matrix (previously matrix $A = (a_{ij})$).}
	\idone{$M = \{1, 2,\dotsc, m\}$ - rows of the matrix, each row $i \in M$ is a strip.}
	\idone{$N = \{1, 2,\dotsc,n\}$ - columns of the matrix, each column $j \in N$ is an item.}
	\idone{$x_{ij} = 1$ iff item $j$ is on strip $i$ (previously $a_{ij} = 1$).}
	\idone{Say that row $i$ covers column $j$ if $x_{ij} = 1$.}
	\idone{Find the smallest number of strips $S \subseteq M$ that contains every item exactly once, i.e. union of strips $= \mathcal{I}$ and intersection $= \emptyset$.}
	\idone{Find minimum cardinality subset $S \subseteq M$ of rows such that each column $j \in N$ is covered by exactly one row $i \in S$.}
\end{itemize}
}

\subsection{Experimental Results - Postoptimisation}
\label{sub:exppostopt}
{\color{myGreen}
\begin{itemize}[leftmargin=*]
	\item Use set $\mathcal{A}$ of feasible packings created during EA in previous section.
	\item C++ circularly linked lists
	\item Designed to not search for all covers, only minimum one, so recursion is restricted to as many levels as the best solution found so far.
	\item Also used Xpress Mosel model (link) to compare.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{Conclusion and Further Work}
\label{sec:conclusion}
{\color{myPurple}
\begin{itemize}[leftmargin=*]
	\item Could use selected packings rather than all packings.
	\item Lower bound.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\begin{comment}
\section{Checklist}
{\color{myAqua}
\begin{itemize}[leftmargin=*]
	\item Vicinal \emph{sum} constraint, not vicinal \emph{score} constraint.
	\item All dashes $'$ not '. 
	\item Citations/references/links.
	\item Abbreviations (SCSPP, SubSCP, AHC etc.) spelling and being used correctly.
	\item Figure and table captions.
	\item References in correct format, title, year, authors, journal etc.
	\item Name of entire process EAX?
	\item Zenodo links DOI.
	\item State specification of computers used for experiments.
	\item Section and subsection titles, capitalisation and spelling.
	\item All figures have same line thickness, dashed line density and thickness, label size, vertex size, and colour (use tikz colours \texttt{tRed} and \texttt{tBlue}).
	\item All figures aligned correctly, subfigures aligned so that the captions are level.
	\item \texttt{$\backslash$noindent} only used when required, after equations, check if needed after definitions/figures/tables etc.
	\item Equations referenced using \texttt{$\backslash$eqref}, not \texttt{$\backslash$ref}.
	\item Tilde $\sim$ before all \texttt{$\backslash$ref} and \texttt{$\backslash$eqref}.
	\item Font/colours of figures clear, labels legible.
	\item American/British spelling.
	\item Word repetitions, duplicate statements.
	\item Footnotes.
	\item Compare with previous paper.
	\item Table footnotes, font, spelling.
	\item Pseudocode?
	\item Dominating $\to$ universal.
	\item Use \texttt{$\backslash$dotsc} in all cases.
	\item Edges in parentheses $()$, not braces $\{\}$.
	\item Check correct notation used, $i, j, S_i, S_j$ etc.
	\item Do not use "OR" or Operational Research" etc.
	\item APA referencing style.
	\item MGPs referenced throughout paper, make sure it is actually stated in the introduction (might be removed from introduction because of new layout from Rhyd's suggestions - 04/04/2019)
	\item EPSRC?
	\item Check things that have been removed from intro are not directly referenced to or are placed elsewhere in the paper (e.g. BPP NP-hard)
	\item Strips $\to$ bins.
	\item SCSPP $\to$ SCPP.
	\item Change thickness of dashed lines tikz figures.
\end{itemize}		
}
\end{comment}

\bibliographystyle{model5-names}
\bibliography{includes/bibliography}

\end{document}