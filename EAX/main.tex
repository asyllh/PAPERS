\documentclass[authoryear]{elsarticle}
\input{includes/preamble.tex}
\begin{document}
	
\begin{frontmatter}
\title{Evolutionary Methods for the Score-Constrained Packing Problem}
\author{Asyl L. Hawa}
\author{Rhyd Lewis}
\author{Jonathan M. Thompson}
\address{School of Mathematics, Cardiff University, Senghennydd Road, Cardiff, UK}
\begin{abstract}
\note{Type of packing problem in which order and orientation of items is important. Paper investigates heuristics, EA, postoptimisation.} 
\end{abstract}	
\end{frontmatter}

%--------------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
\noindent Many problems in operational research and discrete mathematics involve the grouping of elements into subsets. These types of problems can be seen in areas such as scheduling \citep{thompson1998, carter1996}, frequency assignment \citep{aardal2007}, graph colouring \citep{lewis2012, malaguti2008}, and load balancing \citep{rekiek1999}, as well as in practical problems in computer science such as table formatting, prepaging, and file allocation \citep{garey1972}. Formally, given a set $\mathcal{I}$ of $n$ elements, the aim is to produce a partition of subsets $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{i=1}^{k} S_i &= \mathcal{I}, & \label{eqn:allpacked}\\[3pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1, 2,\dotsc,k\}, \hspace{1mm} i \neq j, \text{ and}\label{eqn:nointersect}\\[3pt]
	S_i &\in \mathcal{F} & &\forall \hspace{1mm} i \in \{1,2,\dotsc,k\}.\label{eqn:feasible}
	\end{alignat}
\end{subequations}

\noindent Here, \eqref{eqn:allpacked} and \eqref{eqn:nointersect} state the requirement that every element must be in exactly one of the $k$ subsets and \eqref{eqn:feasible} specifies that each subset $S_i \in \mathcal{S}$ must be feasible, where $\mathcal{F}$ is used to denote the set of all feasible subsets of elements in $\mathcal{I}$. The notion of feasibility is dependent on the particular constraints of the given problem. For example, in the graph colouring problem where vertices on a graph must be assigned colours such that no two adjacent vertices are in the same colour class, $\mathcal{F}$ contains all possible independent sets of vertices, whilst for the classical one-dimensional bin-packing problem (BPP) which requires a set of items of varying sizes to be packed into the fewest number of bins of fixed capacity, a bin $S_i$ is feasible if it is not overfilled.

The focus of this paper is on another packing problem that occurs in the packaging industry, where flat rectangular items of varying widths are to be cut and scored from fixed-length strips of cardboard to then be folded into boxes. This problem was originally introduced as an open-combinatorial problem by \citeauthor{goulimis2004} in 2004, and subsequently studied in \citet{lewis2011}, \citet{becker2015}, and \citet{hawa2018}.

Consider a set $\mathcal{I}$ of $n$ rectangular items of fixed height $H$. Each item $i \in \mathcal{I}$ has width $w_i \in \mathbb{Z}^+$, and is marked with two vertical score lines in predetermined places. The distance between each score line and the nearest edge of the item are the score widths, $a_i, b_i \in \mathbb{Z}^+$ (where w.l.o.g. $a_i \leq b_i$). An example of an item $i$ with these dimensions is provided in Fig.~\ref{fig:itemsdimknives}. Pairs of knives mounted on a bar simultaneously cut along the score lines of two adjacent items, making it easier to fold the cardboard at a later stage. Due to the manner in which the machine is designed, the knives in each pair must maintain a set distance from one another, a so-called ``minimum scoring distance'' $\tau \in \mathbb{Z}^+$, (approximately 70mm in industry). For the knives to score all of the items in the correct locations, the distance between two score lines of adjacent items must be equal to or exceed the minimum scoring distance. Hence, the following \emph{vicinal sum constraint} must be fulfilled:
\begin{equation}
	\textbf{r}(i) + \textbf{l}(i+1) \geq \tau \quad \forall \hspace{1mm} i \in \{1,2,\dotsc,|S|- 1\},
	\label{eqn:vsc}
\end{equation}

\noindent where \textbf{l}($i$) and \textbf{r}($i$) denote the left- and right-hand score widths of the $i$th item in bin $S$. Clearly if this constraint is satisfied the distance between the score lines will be sufficient for the knives to be able to cut appropriately. Figure~\ref{fig:itemsdimknives} shows how adjacent items are scored simultaneously by pairs of knives. Although the vicinal sum constraint is met between items A and B, the full alignment of all three items is infeasible as the sum of the adjacent score widths of items B and C is less than the minimum scoring distance $\tau$, and so the knives are unable to move close enough together to score the lines in the required locations.
\begin{figure}[H]	
	\centering
	\includestandalone[width=0.7\textwidth]{figures/itemsdimknives}
	\caption{Dimensions of an item $i$ marked with dashed score lines, and an example packing showing both feasible and infeasible alignments of three items to be scored by pairs of knives. Here, the minimum scoring distance $\tau = 70$.}	
	\label{fig:itemsdimknives}
\end{figure}

\begin{definition}
	Let $\mathcal{I}$ be a set of $n$ rectangular items of height $H$ with varying widths $w_i$ and score widths $a_i, b_i$ $\forall$ $i \in \mathcal{I}$. Given a minimum scoring distance $\tau$, the Score-Constrained Packing Problem (SCPP) involves packing the items from left to right into the fewest number of $H \times W$ bins such that the vicinal sum constraint is satisifed in each bin and no bin is overfilled.
	\label{defn:scsp}
\end{definition}	

\noindent Each item $i \in \mathcal{I}$ can be packed into a bin in either a regular orientation, denoted $(a_i, b_i)$, where the smaller score width $a_i$ is on the left-hand side of item $i$, or a rotated orientation $(b_i, a_i)$, where the larger score width $b_i$ is on the left-hand side. Thus, the problem involves finding an ordering and orientation of the items in the bins such that the sum of all adjacent score widths is greater than or equal to the minimum scoring distance $\tau$, i.e. the vicinal sum constraint is satisfied.\footnote{Note that the outermost score widths in each bin are disregarded as they are not adjacent to any other items.} For example, in Fig.~\ref{fig:itemsdimknives}, observe that a feasible alignment of all three items can be obtained by rotating item C. As there are $2^{n-1} n!$ distinct orderings of $n$ items, it is clear that enumerative methods are not suitable. Figure~\ref{fig:bppvscpp} shows feasible solutions for a set of items $\mathcal{I}$ as an instance of the BPP and the SCPP, where for the BPP the vicinal sum constraint is disregarded. For the SCPP, an extra bin is required to accommodate all items whilst fulfilling the vicinal sum constraint. It is clear that the solution produced for the BPP is not feasible for the SCPP as the constraint is violated in every bin. Thus the BPP can be seen as a special case of the SCPP when $\tau=0$, as the vicinal sum constraint will always be satisfied.

\begin{figure}[H]
	\centering	
	\begin{subfigure}[h]{0.38\textwidth}
		\includestandalone[width=\textwidth]{figures/bpp}
		\caption{BPP}
		\label{fig:bpp}
	\end{subfigure} \hspace{15mm}
	\begin{subfigure}[h]{0.38\textwidth}
		\includestandalone[width=\textwidth]{figures/scpp}
		\caption{SCPP}
		\label{fig:scpp}
	\end{subfigure}
	\caption{Solutions for the BPP and SCPP using the same set $\mathcal{I}$ of 10 items and $W = 10$. For the SCPP, $\tau = 70$. The red score lines on the solution for the BPP show the vicinal sum constraint violations if it were to be used as a solution for the SCPP.}	
	\label{fig:bppvscpp}
\end{figure}

\noindent One interesting problem related to the BPP is the Trapezoid Packing Problem (TPP), initially investigated by \citet{lewis2011}, where trapezoids are to be packed into bins so as to minimise the number of bins required, whilst also attempting to reduce the amount of triangular waste between adjacent trapezoids. Another problem similar to the BPP is the cutting stock problem (CSP), which involves cutting large pieces of material into smaller piece whilst minimising material wasted. One particular case described by \citet{garraffa2016} considers sequence-dependent cut-losses (SDCL). Here, rectangular items of varying lengths are to be cut from strips of material of fixed lengths, however the type of cutting machine used results in material loss between items during the cutting process. The amount of loss can vary between different items, and is also dependent on the order of the items, i.e. a cut loss between two adjacent items A and B, with A packed first, may not necessarily be equal to the cut loss that arises when B is packed first. Hence, the CSP-SDCL involves packing the items into the fewest number of bins such that the sum of item lengths \emph{and} the sum of cut losses between all adjacent items in each bin does not exceed the bin capacity.

As with the TPP and CSP-SDCL, the SCPP not only involves deciding which bin each item should be packed into, but also, unlike the BPP, \emph{how} the items should be packed -- that is, determining the order and orientation of items within each bin. One specific difference, however, concerns the feasibility of individual bins. In the TPP, although clearly not optimal, it is still legal to place trapezoids with opposite angles, i.e. `$\backslash$' and `/', alongside one another. Likewise in the CSP-SDCL, two items with a large cut loss between them can still be packed alongside one another if necessary. Both of these problems allow items to be packed in \emph{any} order and orientation as long as the bins are not overfilled. In contrast, the SCPP possesses the strong vicinal sum constraint which if violated immediately causes an alignment of items in a bin to be invalid, thus rendering the entire solution infeasible. For consistency, we shall refer to the problem of finding a legal sequence of a subset of items $\mathcal{I}' \subseteq \mathcal{I}$ in a single bin as the Score-Constrained Packing Sub-Problem (SubSCP).

In the next section, we will provide a brief overview of the polynomial-time algorithm used to solve the SubSCP. Section~\ref{sec:scpp} will explain the difficulties associated with the SCPP and analyse heuristics currently in literature. An evolutionary algorithm for the SCPP is presented in Section~\ref{sec:ea}, along with results from rigorous experiments. Section~\ref{sec:postopt} details a postoptimisation procedure involving a recursive backtracking algorithm to improve upon results obtained from the EA, and finally Section~\ref{sec:conclusion} concludes the paper and discusses outcome and possible directions for further work.

%--------------------------------------------------------------------------------------
\section{Solving the SubSCP}
\label{sec:ahc}
\noindent Firstly, let us consider the following sequencing problem originally defined by \citet{hawa2018}:

\begin{definition} % COP
	\label{defn:cop}
	Let $\mathcal{M}$ be a multiset of unordered pairs of integers $\mathcal{M} = \{\{a_1, b_1\}, \{a_2, b_2\},\dotsc,\{a_n, b_n\}\}$, and let $\mathcal{T}$ be a sequence of the elements of $\mathcal{M}$ in which each pair is a tuple. Given a fixed value $\tau \in \mathbb{Z}^+$, the Constrained Ordering Problem (COP) consists of finding a solution $\mathcal{T}$ such that the sum of adjacent values from different tuples is greater than or equal to $\tau$.
\end{definition}

\noindent For example, given the instance $\mathcal{M} = \{\{44,57\}, \{61,48\}, \{4,21\}, \{39,32\}, \{17,29\}, \{26,13\}, \{9,53\}, \{35,41\} \}$ and $\tau = 70$, one possible solution is $\mathcal{T} = \langle(4,21), (53,9), (61,48), (26,13), (57,44), (32,39), (35,41), (29,17)\rangle$. It is evident that the COP is in fact equivalent to the SubSCP, whereby each pair in $\mathcal{M}$ can be seen as an item $i$ represented by its score widths $a_i, b_i$, and the constraint value $\tau$ is the minimum scoring distance. It follows that the requirement for the sum of adjacent values to exceed $\tau$ corresponds to the vicinal sum constraint \eqref{eqn:vsc}.

In this section we present the Alternating Hamiltonian Construction (AHC) algorithm, a polynomial-time algorithm for solving the COP, and hence the SubSCP. The underlying algorithm was originally proposed by \citet{becker2010}, and determines whether a feasible solution exists for a given instance. This was then extended by \citet{hawa2018} so that if a solution does indeed exist AHC is able to construct the final solution. Here, we further simplify and increase the efficiency and speed of AHC by replacing two of the subprocedures used in the previous version of AHC with a single algorithm.

We begin by modelling a given instance $\mathcal{M}$ of the COP as a vertex-weighted graph $G$, which has a vertex set $V$ defined using one vertex for each element in $\mathcal{M}$ in non-decreasing order. Each vertex $v_i \in V$ is weighted with the value in $\mathcal{M}$ it represents. To prevent executing the algorithm unnecessarily, a basic preliminary test is performed to determine if the instance is infeasible. Of the $2n$ vertices, suppose the smallest vertices $v_1$ and $v_2$ are placed on the ends of the sequence. Clearly, if the next-smallest vertex $v_3$ and the largest vertex $v_{2n}$ do not meet the vicinal sum constraint, then there cannot exist a feasible ordering of all elements in $\mathcal{M}$. Note that a positive outcome from this test does not necessarily imply that a feasible solution exists for the instance, however a negative outcome confirms the non-existence of a solution.

If $\mathcal{M}$ has not been deemed infeasible an extra pair of vertices $v_{2n+1}, v_{2n+2}$ is added to $G$, each assigned a weight equal to $\tau$. $G$ now comprises two edge sets: $B$, which contains edges between vertices that are \emph{partners}, that is, whose weights make up an unordered pair in $\mathcal{M}$; and $R$, containing edges between vertices that add up to $\geq \tau$ and are not partners. The additional vertices are partners, and can be seen to be universal vertices as they are adjacent to every other vertex via an edge in $R$; thus both have degree $2n+1$. Given the bijective function $p : V \to V$ that associates each vertex $v_i \in V$ with its partner $p(v_i)$, the set of edges between partners can be denoted as $B = \{(v_i, p(v_i)) : v_i \in V\}$. Note that $|B| = n+1$, and so $B$ is a perfect matching in all cases. Figure~\ref{fig:threshold} illustrates the resulting graph $G = (V, B \cup R)$ produced from the example instance $\mathcal{M}$ of the COP provided above. The graph has a noticeable pattern, with the degree of each vertex increasing in accordance with the weight of the vertices. \ahc{Degree sequence.}

A Hamiltonian cycle in a graph $G$ is a cycle that visits every vertex of $G$ exactly once. A graph containing such a cycle is said to be Hamiltonian. From this, we define a specific type of Hamiltonian cycle:

\begin{definition} % Alternating Hamiltonian Cycle
	\label{defn:althamcycle}
	Let $G = (V, B \cup R)$ be a simple, undirected graph where each edge is a member of one of two sets, $B$ or $R$. $G$ contains an alternating Hamiltonian cycle if there exists a Hamiltonian cycle whose successive edges alternate between sets $B$ and $R$.
\end{definition}

\noindent An alternating Hamiltonian cycle in $G$ corresponds to a legal sequence of the elements in $\mathcal{M}$, as the edges in $B$ represent each pair of values in $\mathcal{M}$, and edges from $R$ depict the values that meet the vicinal sum constraint and can be ordered so that they are adjacent to one another. The solution $\mathcal{T}$ must contain all elements from the problem instance $\mathcal{M}$, so all $n+1$ edges in $B$ must be present in the alternating Hamiltonian cycle. Therefore, the aim is to find a suitable set of edges $R' \subseteq R$ that, together with the edges in $B$, form an alternating Hamiltonian cycle in $G$. The universal vertices aid the construction of the alternating Hamiltonian cycle as they are able to connect to the smallest vertices, however once a cycle has been produced these vertices and any incident edges are removed, resulting in a path corresponding to a solution $\mathcal{T}$.

Determining whether a graph is Hamiltonian is one of the well-known 21 NP-complete problems \cite{karp1972}, whilst the problem of actually finding a Hamiltonian cycle is NP-hard. Consequently, the alternating Hamiltonian cycle problem is also NP-hard, as it is a generalisation of the Hamiltonian cycle problem \citep{haggkvist1977}. Despite this, due to the manner in which graphs are modelled from instances of the COP and the requirement that all edges in $B$ must be included, we are able to determine the existence of an alternating Hamiltonian cycle in a graph $G$ in polynomial time \citep{hawa2018}.

\subsection{The Alternating Hamiltonian Construction Algorithm}
\label{sub:ahc}
\noindent AHC consists of two subprocedures: one to find an initial matching $R' \subseteq R$, and another to modify $R'$ so that it contains suitable edges that form an alternating Hamiltonian cycle with the fixed edges $B$ in $G$. We now describe these two methods in detail.

The first subprocedure of AHC is the Maximum Cardinality Matching (MCM) algorithm, which is used to produce a matching $R'$ from $R$ \citep{mahadev1994}. MCM takes each vertex $v_1, v_2,\dotsc,v_{2n+2}$ and adds to $R'$ the edge from $R$ connecting $v_i$ to the largest vertex $v_j$ that is not incident to an edge in already in $R'$. Pairs of vertices incident to edges in $R'$ are then said to be \emph{matched}. As with partners, the bijective function $m : V \to V$ associates with each vertex $v_i \in V$ its match, $m(v_i)$, thus the set can then be denoted as $R' = \{(v_i, m(v_i)): v_i \in V\}$. In the event that a vertex $v_i$ is not adjacent to any other vertex via an edge in $R$, the previous vertex $v_{i-1}$ can be rematched, provided 
\begin{enumerate*}[label={(\alph*)}]
	\item $i \neq 1$;
	\item $v_{i-1}$ has been matched; and
	\item $(v_{i-1}, p(v_i)) \in R$.
\end{enumerate*} 
Then, we simply set $m(v_i) = m(v_{i-1})$, and $m(v_{i-1}) = p(v_i)$. \ahc{Blossom?}

If $R'$ does not contain $n+1$ edges then there are too few edges to form a cycle with the edges in $B$, thus no feasible solution can exist and AHC is terminated. Otherwise, the spanning subgraph $G'=(V, B \cup R')$ is a 2-regular graph consisting of cyclic components $C_1,C_2,\dotsc,C_z$, as illustrated in Fig.~\ref{fig:mps}. Clearly, if $z = 1$, then $G'$ is an alternating Hamiltonian cycle and a solution has been found. However, if $G'$ comprises multiple cycles, then AHC must find a way of connecting these components together to form a single alternating Hamiltonian cycle.

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.33\textwidth}
		\includestandalone[width=\textwidth]{figures/threshold}
		%\vspace{0.01mm}
		\caption{$G = (V, B \cup R)$}
		\label{fig:threshold}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.33\textwidth}
		\includestandalone[width=\textwidth]{figures/matching}
		%\vspace{0.01mm}
		\caption{$G' = (V, B \cup R')$}
		\label{fig:matching}
	\end{subfigure} \hspace{7mm}
	\begin{subfigure}[h]{0.2\textwidth}
		\includestandalone[width=\textwidth]{figures/mps}
		\caption{$G'$ in planar form}
		\label{fig:mps}
	\end{subfigure}
	\caption{(a) Graph $G$ modelling example instance $\mathcal{M}$; (b) $G'$ formed using $R' \subseteq R$ produced by MCM; and (c) $G'$ in planar form, clearly showing $z = 4$ components. Here, thicker blue edges are in $B$, and thinner red edges are in $R$, with the vertices' weights stated in parentheses.}
	\label{fig:mcm}
\end{figure}

\noindent Recall that an edge in a graph is a \emph{bridge} if removing the edge increases the number of components of the graph, thus the addition of a bridge decreases the number of components. To combine the components of $G'$ into a single component AHC calls upon a second subprocedure, the Bridge-Cover Recognition (BCR) algorithm, an iterative procedure that forms a collection of sets $\mathcal{R}^* = \{R^*_1, R^*_2, \dots\}$ containing specific edges from $R'$. These edges will be replaced by new edges from $R\backslash R'$ that can function as bridges between the different components. $\mathcal{R}^*$ is said to \emph{cover} a component $C_j$ if there exists a set $R^*_i \subset \mathcal{R}^*$ that contains an edge in $C_j$. BCR aims to create a collection $\mathcal{R}^*$ that covers all $z$ components of $G'$.

Firstly, the edges in $R'$ are sorted into a list $\mathcal{L}$ such that the smaller vertices of the edges are in increasing order and the larger vertices are in decreasing order, and any edges that cannot be sorted in this manner are removed from the list. An example of this list can be seen in Fig.~\ref{fig:bcrlist}. During each iteration, BCR searches from the beginning of $\mathcal{L}$ to find successive edges that meet the following conditions:
\begin{enumerate*}[label={(\roman*)}]
	\item the smaller vertex of each edge is adjacent to the larger vertex of the next edge in $\mathcal{L}$;\label{item:adj}
	\item each edge is in a different component of $G'$; \label{item:diffcomp} and
	\item only one of the edges is in a component already covered by $\mathcal{R}^*$, and all other edges are in components not yet covered by $\mathcal{R}^*$.\label{item:overlap}
\end{enumerate*} 
These edges form a set $R^*_i$ which BCR adds to $\mathcal{R}^*$ before continuing through the list in search of another suitable set of edges.\footnote{When searching for edges to produce the first set, $R^*_1$, only conditions \ref{item:adj} and \ref{item:diffcomp} are required as $\mathcal{R}^* = \emptyset$.} After the penultimate edge in the list has been assessed, edges in $\mathcal{R}^*$ are removed from $\mathcal{L}$ and the next iteration begins. Once $\mathcal{R}^*$ covers all components of $G'$, BCR ends the search and proceeds to obtain the bridges using the sets in $\mathcal{R}^*$. If no new sets are created during an iteration and $\mathcal{R}^*$ does not cover all components, then no more sets exist and BCR terminates. Furthermore, if fewer than two edges remain in $\mathcal{L}$ after an iteration, then no more sets can be produced as at least two edges are required to form a new set. In both of these cases, it can be said with absolute certainty that no feasible solution exists for the given instance $\mathcal{M}$ of the COP.

BCR procures the bridges from a collection $\mathcal{R}^*$ covering all components of $G'$ by operating on each set $R^*_i \subset \mathcal{R}^*$ as follows: for each edge in $R^*_i$ in turn, the edge from $R \backslash R'$ connecting the smaller vertex of the edge to the larger vertex of the next edge is added to $R'$. These edges are bridges, connecting the different components together. The edges in $\mathcal{R}^*$ are then removed from $R'$, so that $|R'| = n+1$. Figure~\ref{fig:bcr} shows how BCR performs on our example instance, where the sets $R^*_1 = \{(v_2, v_{17}),(v_3, v_{16}), (v_4, v_{15})\}$  and $R^*_2 = \{(v_7, v_{12}), (v_8, v_{11})\}$ of successive edges in $\mathcal{L}$ that meet the required conditions have been formed. As $\mathcal{R}^* =\{R^*_1, R^*_2\}$ covers all four components of $G'$ no more sets are required, and BCR uses $\mathcal{R}^*$ to acquire the bridges from $R\backslash R'$, as demonstrated in Fig.~\ref{fig:bcrlist}. These bridges are added to the set $R'$, linking the components of $G'$, and replace the edges in $R'$ that appear in $\mathcal{R}^*$, shown in Figs.~\ref{fig:mpsconnect}--\ref{fig:mpscycle}. This modified matching $R'$ is able to form an alternating Hamiltonian cycle in $G'$ with the edge set $B$. Removing the universal vertices yields an alternating Hamiltonian path which corresponds to a feasible solution $\mathcal{T}$ (Fig.~\ref{fig:solutionpath}).

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bcr}
		\caption{}
		\label{fig:bcrlist}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpsconnect}
		\caption{}
		\label{fig:mpsconnect}
	\end{subfigure} \hspace{7mm} %
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpscycle}
		\caption{}
		\label{fig:mpscycle}
	\end{subfigure}
	\begin{subfigure}[h]{0.75\textwidth}
		\includestandalone[width=\textwidth]{figures/solutionpath}
		\caption{}
		\label{fig:solutionpath}
	\end{subfigure}
	\caption{BCR creates a collection $\mathcal{R}^* = \{R^*_1, R^*_2\}$ of edges in $R'$ that when replaced by bridges from $R\backslash R'$ connects the components of $G'$ into a single alternating Hamiltonian cycle. Dashed green edges and dotted orange edges are the bridges from $R^*_1$ and $R^*_2$ respectively. The resulting alternating Hamiltonian path corresponds to a solution $\mathcal{T}$.}
	\label{fig:bcr}
\end{figure}

\noindent In the first algorithm \citep{becker2010}, a procedure is used that searches through $\mathcal{L}$ just once to find edges sets for the collection $\mathcal{R}^*$. For some instances, although $\mathcal{R}^*$ covers all components of $G'$, the components are unable to be connected into a single alternating Hamiltonian cycle. The issue stems from the requirements for edges to form a set, where in this procedure condition~\ref{item:overlap} allows edges to be in multiple components already covered by $\mathcal{R}^*$. This causes additional bridges to be added between components that have already been connected, resulting in multiple components being formed. By restricting condition~\ref{item:overlap} such that only \emph{one} edge can be in a component that $\mathcal{R}^*$ covers, we prevent unecessary edges being added to $G'$ and ensure that components are linked to produce a single cycle. Figure~\ref{fig:overlaperror} shows the formation of $\mathcal{R}^*$ using the original procedure, where the set $R^*_2$ contains edges in \emph{two} components that $\mathcal{R}^*$ already covers. Although $\mathcal{R}^* = \{R^*_1, R^*_2\}$ covers all components of $G'$, the bridges obtained from these sets link $C_2$ and $C_3$ twice, thus connecting the four components into two different components. An additional procedure is implemented by \citet{hawa2018} that recitifies this issue, but we found that the combination of both procedures was unnecessary. Therefore, we replaced the two procedures with a single algorithm, BCR, that produces the same results in a more efficient manner. 

\begin{figure}[H]
	\centering	
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/bcrerror}
		\caption{}
		\label{fig:bcrerror}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpsconnecterror}
		\caption{}
		\label{fig:mpsconnecterror}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.25\textwidth}
		\includestandalone[width=\textwidth]{figures/mpscycleerror}
		\caption{}
		\label{fig:mpscycleerror}
	\end{subfigure}
	\caption{Issue caused using the original procedure to find suitable edge sets, where the collection $\mathcal{R}^*$ covers all components of $G'$, but the bridges obtained from the edge sets in $\mathcal{R}^*$ form two components, as opposed to a single alternating Hamiltonian cycle.}	
	\label{fig:overlaperror}
\end{figure}

\begin{theorem}
	\label{thm:ahc}
	Let $G=(V, B \cup R)$ be a graph modelled from an instance $\mathcal{M}$ of cardinality $n$ of the COP. Then, AHC terminates in at most $O(n^2)$ time.
\end{theorem}

\begin{proof}
	The first subprocedure, MCM, produces an initial matching $R' \subseteq R$ in at most $O(n \lg n)$ time due to the sorting of the vertices. Forming the list $\mathcal{L}$ in the second subprocedure, BCR, also requires $O(n \lg n)$ time. Since each set $R^*_i$ must contain at least two edges, and $\mathcal{L}$ initial consists of up to $n+1$ edges, BCR can form up to $\frac{n+1}{2}$ sets. As $G'$ comprises a maximum of $\frac{n+1}{2}$ components, it follows that the number of edge in $\mathcal{R}^*$ required to cover all components of $G'$  is bounded by $\frac{n+1}{2}-1$. Removing edges from $\mathcal{L}$ can be performed in linear time. At least one set $R^*_i$ is created in each iteration of BCR, therefore producing the collection $\mathcal{R}^*$ is of quadratic complexity $O(n^2)$. Up to $n+1$ edges in $R'$ can be replaced by edges from $R\backslash R'$, and so can be executed in $O(n)$ time. Consequently, AHC has an overall worst case complexity of $O(n^2)$.
\end{proof}	

%--------------------------------------------------------------------------------------
\section{Heuristics for the SCPP}
\label{sec:scpp}
\noindent A feasible solution for an instance of the SCPP is represented by the set $\mathcal{S} = \{S_1, S_2,\dotsc,S_k\}$ such that
\begin{subequations}
	\begin{alignat}{2}
	\bigcup\nolimits_{j=1}^{k} S_j &= \mathcal{I}, & \label{eqn:packall}\\[4pt]
	S_i \cap S_j &= \emptyset &\quad &\forall \hspace{1mm} i, j \in \{1,2,\dotsc,k\}, \hspace{1mm} i \neq j, \label{eqn:nooverlap} \\[4pt]
	A(S_j) = \sum\nolimits_{i=1}^{|S_j|}w_i &\leq W &\quad &\forall \hspace{1mm} S_j \in \mathcal{S}, \label{eqn:capacity} \\[4pt]
	\textup{\textbf{r}}(i) + \textup{\textbf{l}}(i+1) &\geq \tau &\quad &\forall \hspace{1mm} i \in \{1, 2,\dotsc,|S_j|-1\}, \hspace{1mm} \forall \hspace{1mm} S_j \in \mathcal{S}. \label{eqn:vscbin}
	\end{alignat}
\end{subequations}

\noindent An optimal solution for the SCPP is a solution consisting of the fewest number of bins required to feasibly pack all items in $\mathcal{I}$, thus the aim is to minimise $k$. Here, a bin $S_j \in \mathcal{F}$ if the total width of items in the bin does not exceed the bin's capacity \eqref{eqn:capacity} and the vicinal sum constraint is fulfilled \eqref{eqn:vscbin}.

The BPP is known to be NP-hard \citep{garey1979}, and since the SCPP generalises the BPP it follows that the SCPP is also NP-hard. Assuming $P \neq NP$, we cannot hope to find an optimal solution for all instances of the SCPP in polynomial time. The simplest methods to implement for such problems are heuristics, which trade optimality for speed. One of the most well-known heuristics for the BPP is First-Fit (FF), a greedy online algorithm that packs each item, given in some arbitrary order, into the lowest-indexed bin such that the capacity of the bin is not exceed, opening a new bin when required. It is known that there always exists at least one ordering of the items such that FF produces an optimal solution \citep{lewis2009}. An improvement on FF yields the First-Fit Decreasing (FFD) heuristic, which initially sorts the items in non-increasing order of size. In 2007, it was proven that the worst case for FFD is $\frac{11}{9}k + \frac{6}{9}$, and that this bound is tight \citep{dosa2007}. Due to the initial sorting of the items, the time complexity of FFD is $O(n \lg n)$. Similar heuristics include Best-Fit (BF), in which each item is packed into the fullest bin that can accommodate the item without being overfilled, and its offline counterpart Best-Fit Decreasing (BFD). A comprehensive overview of these heuristics and related methods can be seen in \citet{coffman1984}. More advanced heuristics for the BPP have been developed with positive results, such as the Minimum Bin Slack (MBS) heuristic \citep{gupta1999}, which focuses on packing each bin in turn rather than each item, and modifications of MBS such as the Perturbation-MBS' heuristic of \citet{fleszar2002}. 

For the BPP, a basic lower bound for $k$ is the theoretical minimum, $t = \ceil{\sum_{i=1}^{n} w_i / W}$ \citep{martello1990l}, however $t$ will not perform as accurately for the SCPP. Given a set of $n$ items in which the largest score width $b_i < \tau / 2$, it is clear that no pairs of score widths can fulfil the vicinal sum constraint and so each item must be packed into individual bins, thus $|\mathcal{S}| = n$. The theoretical minimum $t$ does not consider the effect of the minimum scoring distance on the feasibility of the solution.

The vicinal sum constraint also introduces further differences in solutions for the BPP and SCPP. The obvious disparity is that of the ordering and orientation of the items in the bins: unimportant in the BPP, but vital for the feasibility of a solution for the SCPP. Another distinction arises when attempting to modify solutions. A solution for the BPP remains feasible when an item is removed or a new item is added to a bin (provided the bin can accommodate said item), whereas this may render a solution for the SCPP infeasible, as the new adjacent score widths may not abide by the vicinal sum constraint. As a result of these differences, basic heuristics designed for the BPP cannot be used for the SCPP as there is no guarantee that the final solution will be feasible.  An example of this issue is provided in Fig.~\ref{fig:ffd} where a solution for an instance of the SCPP has been generated using FFD. The heuristic's inablity to rotate or reorder items produces alignments of items that violate the vicinal sum constraint in three of the bins, thus resulting in an entirely infeasible solution. Notice however that heuritics for the BPP can still produce feasible, albeit not optimal, solutions for the TPP and CSP-SDCL, as the only hard constraint in these problems is the requirement that no bin is overfilled~\eqref{eqn:capacity}.

\begin{figure}[h]	
	\centering
	\includestandalone[width=0.35\textwidth]{figures/ffd}
	\caption{An infeasible solution produced using FFD for an example instance of the SCPP. Here, $|\mathcal{I}| = 15$, $W = 1000$, $\tau = 70$, and the theoretical minimum $t = 6$.}	
	\label{fig:ffd}
\end{figure}

\noindent As the SCPP is a relatively new problem, few methods have been seen in literature. Some basic heuristics were introduced in \citet{hawa2018}, two of which are based on the FFD heuristic for the BPP. The first, named the Modified First-Fit Decreasing (MFFD) heuristic, performs in the same fashion as FFD with the extra condition that an item $i$ can only be packed into a bin $S_j$ if the score width on the end of $S_j$ and one of the score widths $a_i$ or $b_i$ meet the vicinal sum constraint. An improvement on MFFD is their second heuristic, MFFD$^+$, which incorporates AHC. Rather than attempting to pack each item into the ends of bins, MFFD$^+$ calls upon AHC to find a feasible ordering of all items currently in a given bin \emph{and} the item $i$ to be packed, i.e. AHC is used for every instance of SubSCP.\footnote{If an item $i$ is the first to be packed into a bin, it is placed in a regular orientation, $(a_i, b_i)$.} Clearly MFFD$^+$ is the superior of the two, as the application of AHC guarantees that a feasible ordering will be found if it exists. The restriction on MFFD of only packing items in the ends of bins has the potential to increase the number of bins being used in a final solution unnecessarily. Figure~\ref{fig:mffdvsmffdplus} compares solutions produced by MFFD and MFFD$^+$ using the same instance of the SCPP as seen in Fig.~\ref{fig:ffd}. Note how MFFD$^+$ is able to form a solution using the same number of bins as FFD while still satisfying the vicinal sum constraint.

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/mffd}
		\caption{MFFD}
		\label{fig:mffd}
	\end{subfigure} \hspace{15mm}
	\begin{subfigure}[h]{0.35\textwidth}
		\includestandalone[width=\textwidth]{figures/mffdplus}
		\caption{MFFD$^+$}
		\label{fig:mffdplus}
	\end{subfigure}
	\caption{Solutions using MFFD and MFFD$^+$ for the same instance of the SCPP as in the previous figure, where MFFD$^+$ has generated a feasible solution using the same number of bins as FFD.}
	\label{fig:mffdvsmffdplus}
\end{figure}


\subsection{Experimental Results - Heuristics}
\label{sub:expheuristics}
\noindent Although a comparison of these heuristics is given in \citet{hawa2018}, we performed out own tests where AHC used in MFFD$^+$ includes the preliminary test and the new BCR procedure. For our experiments, we produced two different types of instances: ``artificial'', in which the items are strongly heterogeneous (the items have varying widths and score widths); and ``real'', where items are weakly hetergeneous (many items have the same dimensions). For each type 1000 instances were generated using sets of 100, 500, and 1000 items, giving a total of 6000 problem instances. The minimum scoring distance $\tau$ was set to 70mm for all instances - the industry standard. All items have widths $w_i \in [150,1000]$ and score widths $a_i, b_i \in [1,70]$ selected uniform randomly, and equal height $H=1$. Two different bin sizes, $W = 2500$ and 5000 (also for height $H=1$) were used to alter the number of items per bin. \scpp{As optimal solutions are not available, we calculated the solution quality $q = |\mathcal{S}|/ t$ which compares each solution to the theoretical minimum.} Table~\ref{table:heuristics} contains the results from the experiments. All instances were completed in under 125ms.

\begin{table}[h!]
\centering
\caption{\scpp{MFFD vs MFFD$^+$}}
\begin{threeparttable}
\begin{tabular}{c@{\hspace{20pt}}c@{\hspace{20pt}}c@{\hspace{25pt}}c@{\hspace{20pt}}c@{\hspace{10pt}}cc@{\hspace{20pt}}c@{\hspace{10pt}}}\toprule
	& & & \multicolumn{2}{c}{MFFD} &\phantom{abc}& \multicolumn{2}{c}{MFFD$^+$}\\
	\cmidrule{4-5} \cmidrule{7-8}
	Type, $n$ & $W$ & $t$\tnote{$a$} & $|\mathcal{S}|$\tnote{$b$} & $\# t$\tnote{$c$} && $|\mathcal{S}|$ & $\# t$\\ \midrule	
	artificial, 100 & 2500 & 23.323 & 30.754 & 0 && 28.457 & 26 \\
	artificial, 100 & 5000 & 11.922 & 23.583 & 0 && 19.881 & 7 \\
	\midrule
	artificial, 500 & 2500 & 114.942 & 140.206 & 0 && 132.647 & 0 \\
	artificial, 500 & 5000 & 57.722 & 103.209 & 0 && 89.544 & 0 \\
	\midrule
	artificial, 1000 & 2500 & 229.437 & 271.919 & 0 && 258.388 & 0 \\
	artificial, 1000 & 5000 & 114.965 & 198.325 & 0 && 172.613 & 0 \\
	\midrule
	\midrule
	real, 100 & 2500 & 23.473 & 37.069 & 5 && 35.419 & 16 \\
	real, 100 & 5000 & 11.981 & 32.348 & 1 && 29.611 & 5 \\
	\midrule
	real, 500 & 2500 & 115.239 & 184.106 & 0 && 177.249 & 0 \\
	real, 500 & 5000 & 57.865 & 163.819 & 0 && 153.416 & 0\\
	\midrule
	real, 1000 & 2500 & 229.946 & 368.453 & 0 && 355.042 & 0 \\
	real, 1000 & 5000 & 115.227 & 328.618 & 0 && 308.642 & 0 \\
	\bottomrule
\end{tabular}
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
\end{tablenotes}	
\end{threeparttable}	
\label{table:heuristics}
\end{table}

It can be seen that MFFD$^+$ produces solutions using fewer bins on average than MFFD for all 12 instance classes. MFFD was only able to find solutions using $t$ bins for two instance classes, whereas MFFD$^+$ produced solutions using $t$ bins for four instances classes. Solutions using fewer bins were found less frequently on average when the bin size is larger. \scpp{The solution quality $q$ for the six instance classes where $W = 5000$ is significantly higher than when $W = 2500$.} A larger bin size means the bins can accommodate more items, however this increases the number of adjacent score widths that must fulfil the vicinal sum constraint. \scpp{Note also that $q$ is higher for real instances in comparison to artificial instances, as due to the reduced variety of item widths and score widths it is difficult to attain feasible packings.} 

Indeed, it may be that optimal solutions were found in many of these instances, as the lower bound for $k$ may actually be higher than the theoretical minimum calculated. These experiments provide clear results that the use of an exact polynomial-time algorithm, AHC, within a heuristic is more powerful than using a simple heuristic alone. In spite of this, MFFD$^+$ lacks the capability of rearranging items \emph{between} bins, that is, once an item has been packed into a bin, it must stay in that bin (although the position of the item in the bin can change due to the use of AHC). It follows that every item packed has an effect on where the unplaced items can be placed, i.e. where an item is packed is dependent on where previous items have been packed. These limitations associated with greedy heuristics lead us to explore other superior methods.

% Notes - Experiments Heuristics
\begin{comment}
{\color{myPink}
\begin{itemize}[leftmargin=*]
	\item Number of types on average real instances.
	\item Number of items per bin.
\end{itemize}
}
\end{comment}

%--------------------------------------------------------------------------------------
\section{An Evolutionary Algorithm for the SCPP}
\label{sec:ea}
\noindent We now introduce an evolutionary algorithm (EA) for the SCPP \ea{to improve on the results of the previous heuristics}. An evolutionary algorithm is a metaheuristic optimisation algorithm inspired by the natural evolutionary process. Candidate solutions to the problem form the initial population, and procedures emulating selection, reproduction, recombination and mutation are used to create the next generation of solutions. This iterated process results in the evolution of the population. Each solution is evaluated based on a specific critera, and individuals which are more suited to the environment are provided more oppportunity to breed while those which are less so are eliminated. EAs have been used for a variety of grouping problems with positive results \citep{lewis2017, falkenauer1996, quiroz2015}. Within the EA framework, we investigate three different recombination operators as well as a local search procedure inspired by \citet{martello1990l}. The AHC algorithm described in Section~\ref{sec:ahc} is also integrated into the EA to solve instances of the SubSCP.

\subsection{Recombination}
\label{sub:xover}
\noindent A recombination operator is used to generate new solutions from an existing population by taking two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, and combining them to create a new offspring solution. The operator determines which elements from each parent should be inherited by the offspring, with the aim of retaining the best characteristics of each parent such that a superior offspring is produced. As described in the previous section, individual items cannot be copied from parent to offspring as this may cause a violation of the vicinal sum constraint. Thus, all of the operators investigated in our EA are designed such that the vicinal sum constraint is always satisfied, ensuring offspring feasibility.

The first operator is based on the grouping genetic algorithm (GGA) of \citet{falkenauer1992}. The bins of the second parent solution $\mathcal{S}_2$ are permuted and two bins $S_i$ and $S_j$ from $\mathcal{S}_2$ are selected randomly (where $1 \leq i < j \leq |\mathcal{S}_2|$). All bins between and including $S_i$ and $S_j$ are inserted into an offspring solution $\mathcal{S}$. GGA then adds to the offspring all bins from $\mathcal{S}_1$ that do not contain items already present in the offspring. Note that the bins chosen from $\mathcal{S}_2$ cannot both be the outermost bins, that is, GGA cannot choose both $i = 1$ \emph{and} $j = |\mathcal{S}_2|$, as doing so would result in all bins from $\mathcal{S}_2$ being copied into the offspring, preventing the addition of any bins from $\mathcal{S}_1$.

The second operator we implemented is the alternating grouping crossover (AGX), and is analogous to that of \citet{quiroz2015}. Starting with the parent solution containing the fullest bin, AGX inserts this bin into an offspring solution $\mathcal{S}$, and bins containing items in $\mathcal{S}$ are removed from the other parent.\footnote{For both AGX and AGX', in the event that both parents contain bins with equal maximum fullness/number of items, the initial parent solution is chosen at random.} The operator then inserts the fullest bin from the modified parent into $\mathcal{S}$ and removes bins from the first parent. AGX continues to alternate between parents, selecting the fullest bin $max_{S_j \in \mathcal{S}_p} (A(S_j))$ (where $\mathcal{S}_p$ is the parent solution under consideration, $p \in \{1,2\}$) until at most $min (|\mathcal{S}_1|,|\mathcal{S}_2|) - 1$ bins have been added to the offspring solution.

Our final operator, AGX$'$, performs in a similar manner to AGX, however rather than choosing the fullest bin to insert into the offspring solution AGX$'$ selects bins containing the most items, $max_{S_j \in \mathcal{S}_p} (|S_j|)$. This method has the ability to preserve bins containing items that are harder to pack along with other items. 

In order to maintain feasibility, the operators remove entire bins containing duplicate items rather than individual items. These bins may also contain items that are not present in the offspring solution. Consequently, on completion of the crossover, the offspring solution $\mathcal{S}$ may not contain all $n$ items and is therefore not yet a full solution. To rectify this, MFFD$^+$ is applied using the missing items to form a partial solution $\mathcal{S}^*$. The partial offspring solution $\mathcal{S}$ and $\mathcal{S}^*$ are then used as input into a local search procedure to create a full feasible offspring solution. Figure~\ref{fig:recomb} shows the offspring solution $\mathcal{S}$ produced from two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, using each of the recombination operators, along with the individual items missing from each offspring.

\begin{figure}[H]	
	\centering
	\begin{minipage}{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/parentS1}
	\end{minipage} \hspace{15mm}
	\begin{minipage}{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/parentS2}
	\end{minipage}
\end{figure}

\begin{figure}[H]	
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/gga}
		\caption{GGA}
		\label{fig:gga}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/agx}
		\caption{AGX}
		\label{fig:agx}
	\end{subfigure} \hspace{5mm}
	\begin{subfigure}[h]{0.3\textwidth}
		\includestandalone[width=\textwidth]{figures/agxdash}
		\caption{AGX$'$}
		\label{fig:agxdash}
	\end{subfigure}
	\caption{Partial offspring solutions and missing items created from parent solutions $\mathcal{S}_1$ and $\mathcal{S}_2$ using different recombination operators. (a) Bins $S_2, S_3$ and $S_4$ are added to the offspring from $\mathcal{S}_2$; (b)--(c) $\mathcal{S}_1$ is the initial parent for both AGX and AGX$'$ as it contains both the fullest bin and the bin with the most items.}
	\label{fig:recomb}
\end{figure}

\subsection{Local Search}
\label{sub:localsearch}
\noindent Our local search method takes in two feasible partial solutions, $\mathcal{S}$ and $\mathcal{S}^*$, permutes the bins of both, and then attempts to move items between the two solutions in four stages: 
\begin{enumerate*}[label={(\roman*)}]
	\item swapping a pair of items from a bin in $\mathcal{S}$ with a pair of items from a bin in $\mathcal{S}^*$;\label{item:pairpair}
	\item swapping a pair of items from a bin in $\mathcal{S}$ with an individual item from a bin in $\mathcal{S}^*$;\label{item:pairsin}
	\item swapping individual items from bins in $\mathcal{S}$ and $\mathcal{S}^*$;\label{item:sinsin} and
	\item moving an item from a bin in $\mathcal{S}^*$ to a bin in $\mathcal{S}$.\label{item:movesin}
\end{enumerate*} 
During stages \ref{item:pairpair}--\ref{item:sinsin}, the width of the item(s) from $\mathcal{S}^*$ must exceed the width of the item(s) from $\mathcal{S}$. Once a swap or move has been perfomed, the procedure immediately moves on to the next stage. This method is repeated until all four stages have been executed in succession with no changes to $\mathcal{S}$ or $\mathcal{S}^*$. Then, MFFD$^+$ is applied to any items remaining in $\mathcal{S}^*$, generating a new feasible partial solution $\mathcal{S}^{**}$, with $|\mathcal{S}^{**}| \leq |\mathcal{S}^*|$. The bins in $\mathcal{S}^{**}$ are then inserted into $\mathcal{S}$ to form a full feasible solution.

This method is based on the dominance criterion of \citet{martello1990l}: if a bin $S_x$ \emph{dominates} a bin $S_y$, then a solution containing $S_x$ will have no more bins than a solution containing $S_y$. The local search procedure is in fact a local search for dominating bins. By moving larger items into $\mathcal{S}$, the fullness $A(S_j)$ of the bins $S_j \in \mathcal{S}$ increases whilst the number of items per bin is maintained or decreases, improving the \ea{quality} of the bins in $\mathcal{S}$. Simultaneously, items moved into $\mathcal{S}^*$ are smaller and therefore easier to repack into bins in $\mathcal{S}$ during stage~\ref{item:movesin}. Variations of this method can be seen in \citet{lewis2009, lewis2017, falkenauer1996, levine2004}, however the addition of the vicinal sum constraint results in fewer changes than seen in these previous implementations. By iterating the stages numerous distinct subsets of items in the bins are produced, generating more possibilities for feasible orderings of items. It can be seen that this procedure cannot increase the number of bins in a solution, and also has the ability to decrease the number of bins.


\subsection{Evolutionary Algorithm Framework}
\label{sub:eaframework}
\noindent Our EA for the SCPP begins by producing candidate solutions to form an initial population, with one solution created using MFFD$^+$ and the rest using MFFR$^+$ (the same method as MFFD$^+$ with the items in a random order). Each solution is mutated before being inserted into the population. The mutation of a solution $\mathcal{S}$ consists of permuting the bins and inserting $1 < r < |\mathcal{S}|$ bins selected randomly from $\mathcal{S}$ into a set $\mathcal{S}^*$. Local search is then executed using these two partial solutions to produce a full feasible solution $\mathcal{S}$. 

Each iteration of the EA involves selecting two parent solutions, $\mathcal{S}_1$ and $\mathcal{S}_2$, from the population at random, applying a recombination operator to produce an offspring solution $\mathcal{S}$, then finally mutating $\mathcal{S}$ before replacing the least fit of the two parents in the population.

A tailored function is used to determine the fitness of a solution, as opposed to simply relying on the number of bins within a solution. The reason for this is two-fold: firstly, given two solutions of equal size, it is impossible to determine the fitter solution based on the number of bins alone. Secondly, we note that the fitness of a solution not only depends on the number of bins used, but also \emph{how} the items are packed into the bins. It is clear that if the bins' capacities are taken advantage of, fewer bins will be needed to pack all items compared to if the bins are only half full, requiring more bins than necessary to accommodate all of the items. A solution comprising fuller bins may also contain bins that are nearly empty, which is beneficial as it allows futher items to be packed, or the residual material could be used for other means. 

We therefore make use of the following function to calculate the fitness of a solution $\mathcal{S}$ \citep{falkenauer1992}:
\begin{equation}
	f(\mathcal{S}) = \frac{\sum_{S_j \in \mathcal{S}} (A(S_j)/W)^2}{|\mathcal{S}|}
\end{equation}

\noindent which assigns higher values to fitter solutions. This function exerts evolutionary pressure by favouring solutions with superior packings, reducing the ability for less fit solutions to reproduce. Note also that if $|\mathcal{S}_1| < |\mathcal{S}_2|$  then $f(\mathcal{S}_1) > f(\mathcal{S}_2)$, hence a global optimum for this fitness function is associated with the optimal solution containing the fewest number of bins.

It is also important to consider how the vicinal sum constraint affects the fitness of a solution. Given two half-full bins containing items from an instance of the classical BPP, it may be possible to move items between the bins such that one bin is almost full and the other nearly empty. Now, suppose the items in the half-filled bins have score lines, and are from an instance of the SCPP. Obviously, it is more difficult to shuffle the items between the bins as the vicinal sum constraint may be violated. Thus, fuller bins in the SCPP are extremely valuable, and the fitness function ensures that solutions containing fuller bins are preserved in the population throughout generations.

% Notes - EA
{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item Iterative procedure, plus local search and xover, swapping items between bins, better than previous heuristics as repetition means more possible item combinations in bins, EA has the ability to move items between bins.
	\item EA consists of multiple iterations, previous heuristics only one.
	\item What is GGA's aim?
	\item How is AGX useful? Choose fullest bin, less waste, less bins required.
\end{itemize}
}

\subsection{Experimental Results - EA}
\label{sub:expea}
\noindent To allow for a fair comparison with the heuristics in Section~\ref{sec:scpp}, the same sets of problem instances from the previous experiments were used to obtain results from our EA. Each of the instance classes were executed six times using different recombination operators and bin sizes, producing 36 subclasses overall. We settled for an initial population containing 25 candidate solutions, which was shown to produce the best results. The advantage of using a small population size was to be expected, as the vicinal sum constaint limits the number of feasible candidiate solutions using a reasonable number of bins. It was seen that larger population sizes contained an abundance of poor solutions comprising over twice the number of bins as used in the final solution, whilst even smaller populations lacked the diversity needed to escape local optima. Across all instances, the EA had a fixed time limit of 300 seconds. Table~\ref{table:ea} displays the results obtained from the experiments.

\begin{table}[h!]
\centering
\caption{\ea{EA comparisons}}
\begin{threeparttable}
\begin{tabular}{c@{\hspace{20pt}}c@{\hspace{20pt}}ccc@{\hspace{15pt}}c@{\hspace{10pt}}cc@{\hspace{15pt}}c@{\hspace{10pt}}cc@{\hspace{15pt}}c@{\hspace{10pt}}}\toprule
	& & & \phantom{a} & \multicolumn{2}{c}{GGA} &\phantom{ab}& \multicolumn{2}{c}{AGX} &\phantom{ab}& \multicolumn{2}{c}{AGX$'$}\\
	\cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
	Type, $n$& $W$ & $t$\tnote{$a$} && $|\mathcal{S}|$\tnote{$b$} & $\# t$\tnote{$c$} && $|\mathcal{S}|$ & $\# t$ && $|\mathcal{S}|$ & $\# t$\\ \midrule \midrule
	artificial, 100 & 2500 & 23.323 && 23.368 & 966 && 23.349 & 981 && 23.36 & 971 \\
	artificial, 100 & 5000 & 11.922 && 12.304 & 833 && 12.352 & 810 && 12.353 & 807 \\
	\midrule
	artificial, 500 & 2500 & 114.942 && 116.694 & 268 && 116.966 & 224 && 116.56 & 291 \\
	artificial, 500 & 5000 & 57.722 && 62.393 & 43 && 62.813 & 35 && 63.022 & 37 \\
	\midrule
	artificial, 1000 & 2500 & 229.437 && 234.938 & 7 && 235.458 & 10 && 234.409 & 13 \\
	artificial, 1000 & 5000 & 114.965 && 127.673 & 0 && 128.037 & 0 && 128.122 & 0 \\
	\midrule \midrule
	real, 100 & 2500 & 23.473 && 26.108 & 551 && 26.146 & 558 && 26.029 & 554 \\
	real, 100 & 5000 & 11.981 && 17.667 & 470 && 17.718 & 458 && 17.692 & 449 \\
	\midrule
	real, 500 & 2500 & 115.239 && 134.642 & 33 && 134.74 & 47 && 134.49 & 36 \\
	real, 500 & 5000 & 57.865 && 94.491 & 81 && 95.009 & 78 && 94.76 & 74 \\
	\midrule
	real, 1000 & 2500 & 229.946 && 270.714 & 0 && 270.862 & 2 && 270.782 & 0 \\
	real, 1000 & 5000 & 115.227 && 193.595 & 26 && 193.957 & 25 && 193.838 & 24 \\
	\bottomrule
\end{tabular}	
\vspace{0.2cm} %
\begin{tablenotes}
	\item[$a$] $t = \ceil{\sum_{i=1}^{n} w_i / W}$ (mean from 1000 instances).
	\item[$b$] Number of bins per solution (mean from 1000 instances).
	\item[$c$] Number of instances where a solution using $t$ bins was found (from 1000 instances).
\end{tablenotes}
\end{threeparttable}
\label{table:ea}
\end{table}

\noindent Firstly, it is clear that the EA outperforms the previous heuristics, producing solutions using $t$ bins in all but one of the 12 subclasses. GGA consistently generates solutions with the fewest number of bins on average when $W=5000$, whilst AGX$'$ is more favourable when $W=2500$. One interesting observation is of the results obtained using AGX. Despite producing solutions with the most bins on average across all real instances, AGX also yields the highest number of solutions comprising $t$ bins when $W=2500$. This suggests a high variance in the quality of solutions.

{\color{myRed}
\begin{itemize}[leftmargin=*]
	\item Better than heuristics - compare.
	\item Artificial
	\begin{itemize}
		\item AGX$'$ best for $W=2500$, GGA best for $W=5000$ -- what does this show?
		\item NSA105, no recombination operator produced a single solution where $|\mathcal{S}| = t$.
		\item As $n$ increases, difference between $t$ and average $|\mathcal{S}|$ increases, i.e. for NSA12, difference is between 0.045 -- 0.026, whereas for NSA105, difference is between 13.157 -- 12.708.
		\item Number of solutions where $|\mathcal{S}| = t$ decreases as $n$ increases.
		\item Number of solutions where $|\mathcal{S}| = t$ less when $W=5000$ than when $W=2500$ for same number of items $n$.
		\item Difference between average $|\mathcal{S}|$ and $t$ smaller when $W=2500$ than when $W=5000$ for same number of items $n$.
	\end{itemize}
	\item Real
	\begin{itemize}
		\item GGA better when $W=5000$.
	\end{itemize}	
	\item Time graph output.
	\item Number of EA iterations within time limit, how does it vary between recombination operators?
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{Postoptimisation}
\label{sec:postopt}
\noindent Given a collection $\mathcal{S}$ of subsets of a set $X$, an \emph{exact cover} is a subcollection $\mathcal{S}^*$ of $\mathcal{S}$ such that each element in $X$ is contained in exactly one subset in $\mathcal{S}^*$.

Consider an $m\times n$ binary matrix $X$. Let $M = \{1,2,\dotsc,m\}$ and $N = \{1,2,\dotsc,n\}$ be the rows and columns of the matrix respectively. Now, suppose each row $i \in M$ represents a bin, and each column $j \in N$ represents an item. Then, an element of the matrix $x_{ij} = 1$ iff item $j$ is in bin $i$. It can be said in this case that row $i$ \emph{covers} column $j$. Thus, the problem is to find a minimum cardinality subset of rows $S \subseteq M$ such that each column $j \in N$ is covered by exactly one row $i \in S$. This problem can be formulated as the following integer linear program.

\begin{subequations}
	\begin{alignat}{3}
		\text{minimise  } &\sum_{i \in M} c_i & \\[3pt]
		\text{subject to  } &\sum_{i \in M} x_{ij} c_i = 1 &\quad &\forall \hspace{1mm} j \in N \\[3pt]
		&c_i \in \{0,1\} & &\forall \hspace{1mm} i \in M
	\end{alignat}
\end{subequations}

\[c_i =
\begin{cases} 
1 & \text{if } i \in S \\
0 & \text{otherwise} 
\end{cases}
\]

\noindent The exact cover problem, determining whether a subcollection $\mathcal{S}^*$ exists, is a decision problem, and one of Karp's 21 NP-complete problems \cite{karp1972}. However, if we know that an exact cover does indeed exist, we can alter the problem to instead find the smallest subcollection.

Given a set of feasible bins, we can use this to find a solution to the SCPP. Since the bins provided are already feasible, the complications associated with the vicinal sum constraint are eliminated.

One method of solving the exact cover problem is by using a recursive depth-first backtracking algorithm, dubbed ``Algorithm X'' by Donald Knuth \cite{knuth2000}, which is used to find all solutions. Given our problem, it is unecessary to find all solutions. Instead, we have adapted the procedure to only seach for solutions that improve upon the best solution found thus far. Ideally, we would use the entire set of feasible bins $\mathcal{F}$ to form the matrix, however this could include hundreds of millions of feasible bins. Instead, we chose to use the set $\mathcal{A}$ created during the EA, which contains feasible bins found during the algorithm.

{\color{myGreen}
\begin{itemize}[leftmargin=*]
	\item Cite \citet{malaguti2008}.
	\item Exact cover formulation, NP-hard, state the IP, describe DLX.
	\item Cite \citet{knuth2000} dancing steps.
	\item Use strips in feasPacking for post opt.
	\item Compare with EA output - is a better solution found, or a solution with the same number of strips but a better fitness value?
	\item Is the post opt phase able to find a solution equal to the lowerbound?
	\item Post opt will only ever find a solution equal to or better than the solution found in EA, never worse.
	\item Execution time.
	\item Set cover problem is optimisation problem, find min number of sets.
	\item Exact cover problem is decision problem, does a set exist.
	\item However since we add every item on its own strip to feasPacking, we know that a set exists.
	\item Problem is to find minimum number of strips that covers all items and contains every item exactly once.
	\idone{$X = (x_{ij})$ - $m$ x $n$ matrix (previously matrix $A = (a_{ij})$).}
	\idone{$M = \{1, 2,\dotsc, m\}$ - rows of the matrix, each row $i \in M$ is a strip.}
	\idone{$N = \{1, 2,\dotsc,n\}$ - columns of the matrix, each column $j \in N$ is an item.}
	\idone{$x_{ij} = 1$ iff item $j$ is on strip $i$ (previously $a_{ij} = 1$).}
	\idone{Say that row $i$ covers column $j$ if $x_{ij} = 1$.}
	\idone{Find the smallest number of strips $S \subseteq M$ that contains every item exactly once, i.e. union of strips $= \mathcal{I}$ and intersection $= \emptyset$.}
	\idone{Find minimum cardinality subset $S \subseteq M$ of rows such that each column $j \in N$ is covered by exactly one row $i \in S$.}
\end{itemize}
}

\subsection{Experimental Results - Postoptimisation}
\label{sub:exppostopt}
{\color{myGreen}
\begin{itemize}[leftmargin=*]
	\item Use set $\mathcal{A}$ of feasible packings created during EA in previous section.
	\item C++ circularly linked lists
	\item Designed to not search for all covers, only minimum one, so recursion is restricted to as many levels as the best solution found so far.
	\item Also used Xpress Mosel model (link) to compare.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\section{Conclusion and Further Work}
\label{sec:conclusion}
{\color{myPurple}
\begin{itemize}[leftmargin=*]
	\item Could use selected packings rather than all packings.
	\item Lower bound.
\end{itemize}
}

%--------------------------------------------------------------------------------------
\begin{comment}
\section{Checklist}
{\color{myAqua}
\begin{itemize}[leftmargin=*]
	\item Vicinal \emph{sum} constraint, not vicinal \emph{score} constraint.
	\item All dashes $'$ not '. 
	\item Abbreviations (SCPP, SubSCP, AHC etc.) spelling and being used correctly.
	\item Figure and table captions.
	\item Check that correct figures are being referred to in text.
	\item Name of entire process EAX?
	\item Zenodo links DOI.
	\item State specification of computers used for experiments.
	\item Section and subsection titles, capitalisation and spelling.
	\item All figures have same line thickness, dashed line density and thickness, label size, vertex size, and colour (use tikz colours \texttt{tRed} and \texttt{tBlue}).
	\item All figures aligned correctly, subfigures aligned so that the captions are level.
	\item \texttt{$\backslash$noindent} only used when required, after equations, check if needed after definitions/figures/tables etc.
	\item Equations referenced using \texttt{$\backslash$eqref}, not \texttt{$\backslash$ref}.
	\item Tilde $\sim$ before all \texttt{$\backslash$ref} and \texttt{$\backslash$eqref}.
	\item Font/colours of figures clear, labels legible.
	\item American/British spelling.
	\item Word repetitions, duplicate statements.
	\item Footnotes, check if required or if statement can be put in text.
	\item Compare with previous paper.
	\item Table footnotes, font, spelling.
	\item Pseudocode?
	\item Dominating $\to$ universal.
	\item Use \texttt{$\backslash$dotsc} in all cases.
	\item Edges in parentheses $()$, not braces $\{\}$.
	\item Check correct notation used, $i, j, S_i, S_j$ etc.
	\item Do not use "OR" or Operational Research" etc.
	\item APA referencing, check references spelling and format, correct titles, years etc.
	\item MGPs referenced throughout paper, make sure it is actually stated in the introduction (might be removed from introduction because of new layout from Rhyd's suggestions - 04/04/2019)
	\item EPSRC?
	\item Check things that have been removed from intro are not directly referenced to or are placed elsewhere in the paper (e.g. BPP NP-hard)
	\item Strips $\to$ bins.
	\item SCSPP $\to$ SCPP.
	\item New citation style, check that sentences make sense, i.e. ``implemented in [1]...'' works but ``implemented in Becker (2010)...'' does not.
	\item Use correct citation command: citet, citep, citeauthor etc.
	\item Fulfil (one `l').
	\item ``red'' and ``blue'' edges, refer to them in figures, thick blue, thin red.
	\item Vertex weights.
	\item Check notation used in tikz figures is correct.
	\item Capital F for Figure in middle of sentence?
	\item Remove custom colours.
	\item Tense, ``the first operator we investigate'' or ``investigated''?
	\item Go through Elsevier Author Guide.
	\item Change all ``Figure'' to ``Fig.''.
\end{itemize}		
}
\end{comment}

\bibliographystyle{model5-names}
\bibliography{includes/bibliography}

\end{document}